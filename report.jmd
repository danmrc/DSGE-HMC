---
title: Hamiltonian Monte Carlo and DSGE
author: Gilberto Boaretto and Daniel Coutinho
date: December, 2020
bibliography: /home/danielc/Documentos/dsge-hmc_V2/DSGE.bibtex
weave_options:
  md2html:
    out_path: reports/out/report.html
  pandoc2html:
    out_path: reports/out/report.html
---

# Introduction

Bayesian methods are frequently used to estimate DSGE models, that generally have irregular likelihood functions and non linearities in the map from structural parameters to the reduce form parameters. @Villaverde2020 discuss the benefits of using Hamiltonian Monte Carlo (HMC) compared to the usual Random Walk Metropolis Hasting (RWMH): since RWMH takes "blind" steps to sample the distribution, the method can be inneficient, spending long periods in certain regions of the desnity and generating highly correlated samples. HMC tries to avoid this by using the gradient of the posterior to take informed steps to the "typical set", a region in the posterior that concentrates most of the mass. Specially in high dimensions, this region is _not_ the region immediately around the mode. HMC visits this region by clevearly balancing the atraction of the mode using as an analogy a physics model - see @Betancourt2017 for details.

The hard part is to get the gradient of the (log)likelihood of the a DSGE model. Most DSGE models are solved numerically, by fairly opaque methods (e.g. Generalized Schur Decomposition) that are not easily differentiable. Fortunanetly, @Iskrev2010 comes with a solution that is independent of the solver. Then, one has to work with the difference of the Kalman Filter, which is more straightforward than differentiating the solution of a DSGE model but requires some careful work due to the recursive structure of the Kalman Filter.

We put all this pieces together to estimate an very simple DSGE model using HMC. Larger models probably will benefit more than our 10 parameter model from @Gali2009. However, our very simple model allows us to debug the model faster than a large model. Since allother pieces are functions, they can be easily reused for a larger model.

The next section talks about Hamiltonian Monte Carlo. We explain all the pieces that allow us to differentiate the loglikelihood of a DSGE model analytically [^1] in section 3. Section 4 shows the results from estimating a 4 equation model using HMC. Section 5 concludes.

# Hamiltonian Monte Carlo in a section

Hamiltonian Monte Carlo is a Markov Chain Monte Carlo algorithm first applied to Statistics by @Neal2011. While RWMH decides which point will be the next to be sampled based on a random walk, HMC uses information from the gradient of the loglikelihood to move the algorithm towards regions with higher density. This needs to be done carefully to avoid (1) distorting the posterior distribution and (2) avoid getting stuck in the mode of the distribution. HMC does that by using Hamiltonian Dynamics from physics and introducing a momentum variable, that act "against" the pull from the gradient. Bethancourt (2017) gives the metaphor of a satelite in orbit: the earth pulls it down while the momentum keeps the satelite in orbit. Hamiltonian Monte Carlo attempts the same:the mode of the distribution

# Derivating the posterior

HMC uses the gradient of the loglikelihood to help select a new draw for the parameters. Most of the likelihoods for DSGE models computes the solution of the model by using one of the many algorithms to do it - like @Sims2002, @Blanchard1980, @Anderson1985 - and then proceed to use the solution to generate a state space model. Let $\theta$ denote the structural parameters. Our state space model will be:

$$
y_t = Gx_t + u_t \\
x_{t+1} = A(\theta)x_t + v_t
$$

In which $u_t$ and $v_t$ have a Gaussian distribution and variance matrices $R$ and $Q(\theta)$, respectively. The rational expectations solver gives A and Q. G normally is a matrix of zeros and ones, and therefore does not depends on $\theta$. The Kalman Filter uses information from the observed variables ($y_t$) to recover the state ($x_t$). We can build the likelihood of the next observation given our information using $\hat{x}_{t+1|t}$, the estimated state for time $t+1$ with information at $t$. Using $\ell$ to denote the loglikelihood and $m$ the number of observables, we have:

$$
\ell(\{y_t\}_{t=1}^T) = \sum_{t=1}^T \ell(y_t|\{y_{\tau}\}_{\tau=1}^{t-1}) = \sum_{t=1}^T -\frac{1}{2}\left(m\log(2\pi) + \log{}\det(B_t) + (y_t - G\hat{x}_{t|t-1})^{\prime}B_t^{-1}(y_t - G\hat{x}_{t|t-1}) \right)
$$

In which $\ell(y_t|\{y_{\tau}\}_{\tau=1}^{t-1})$ is the loglikelihood now conditional to the previous values of $y_t$ and $B_t$ is the variance of y[^2]. Clearly if one wants to differentiate the loglikelihood, then we need to differentiate $\hat{x}_{t|t-1}$ and consequently $A(\theta)$. Even though the model is linear, the map from $\theta$ to $A$ needs not to be linear. @Iskrev2010 uses the implicit function theorem and matrix differentiation rules to get the derivatives of $A$ and $Q$, $dA$ and $dQ$. The procedure is straightforward, and since the reduced form is the same no matter which method you use, the results can be applied to any solver. However, the structural form is written in a very specific way and it is not easy to change that, so the model may have two different structural forms.

Diferentiating the Kalman Filter is a simpler task, and requires the equations for the Kalman Filter and knowing the rules of differentation for matrices, as in @Magnus2019. @Lomba2102 contains all the results. It is important to realize that, given the recursive structure of the filter, some derivates will depend on the previous value of the filter.

HMC works better if there are no restrictions to the parameter value, i.e. every parameter belong to the whole real line. Many parameters have natural restrictions on them - variances are always positive, for an example - and some parameters have restrictions that should be imposed - we would expect that auto regressive shocks are not explosive. All this can be easily handled using transformations. When we do that, we need to change the density by the determinant of Jacobian of the transformations. Since the transformations are set in a parameter by parameter basis, the Jacobian is a diagonal matrix and the determinant is easily calculated.

We have to set which functions are used to deal with each transformation. The table bellow shows our choices:

| Transformation                          |    Function    |
|-----------------------------------------|----------------|
| $\mathbb{R}^{+} \rightarrow \mathbb{R}$ |   $\log(x)$    |
| $[0,1] \rightarrow \mathbb{R}$          | $-\log(1/x-1)$ |

We also code the the derivate of this functions. Now let $g:\Theta \rightarrow \mathbb{R}^{p}$ map from the space of parameters to the Euclidian space of dimension p. We will use $J(g(\tilde{\theta}))$ as the jacobian of $g$. The log likelihood with the irrestrict parameters $\tilde{\theta}$ is:

$$
L(\tilde{\theta}|\{y_{\tau}\}_{\tau=1}^{t-1}) = L(g^{-1}(\theta)|\{y_{\tau}\}_{\tau=1}^{t-1})abs(\det(J(g^{-1}(\tilde{\theta}))))
$$

Let $\mathcal{P}$ be the posterior and $\pi(\theta)$ be the prior of all parameters ($\pi(\theta) = \pi_1(\theta_1)\times \ldots \times \pi_p(\theta_p)$), then:

$$
\mathcal{P}(\theta,y) = \prod_{t=1}^T L(\theta|\{y_{\tau}\}_{\tau=1}^{t-1})\pi(\theta) = \pi(\theta)\prod_{t=1}^T L(\theta|\{y_{\tau}\}_{\tau=1}^{t-1})\\
\mathcal{P}(\tilde{\theta},y) = \pi(g^{-1}(\tilde{\theta}))abs(\det(J(g^{-1}(\tilde{\theta}))))\prod_{t=1}^T L(g^{-1}(\tilde{\theta})|\{y_{\tau}\}_{\tau=1}^{t-1})\\
\log(\mathcal{P}(\tilde{\theta},y)) = \log(\pi(g^{-1}(\tilde{\theta}))) + \log{}\det(abs(J(g^{-1}(\tilde{\theta})))) + \sum_{t=1}^{T} \ell(g^{-1}(\tilde{\theta})|\{y_{\tau}\}_{\tau=1}^{t-1})
$$

And we need to differentiate this to get the gradient. Notice that we need to differentiate $abs$, which is not differentiable. However, it is a convex function and we can define a sub derivative. We use automatic differentiation to deal with it, and its definition for the derivative of $abs$ is the sign function.

# The results

All the program is implemented in Julia, a relatively new programming language focused on scientific programming. We wrote almost everything from the ground up, except the HMC sampler[^3]. HMC samplers come in different flavours and we use No U Turn Sampler (NUTS), by @Gelman2014. This allows us to control every single aspect of the program. Everything is implemented in pure Julia. We will show the result from the estimation bellow. We use 1000 samples for burn in and 2000 samples to generate the graphics bellow. The model is the model from @Gali2009, and we provide a handy table for the parameter:

|   Letter  |             Structural Parameter               | Value |
|-----------|------------------------------------------------|-------|
| $\beta$   | Discount Factor                                | 0.99  |
|$\epsilon$ | Elasticity of the varieties                    |   6   |
|$\theta$   | Probability of the firm keep the price fixed   |  2/3  |
|$\sigma$   | CRRA Parameter for consumption                 |   1   |
|  $\phi$   | CRRA Parameter for work                        |   1   |
|$\sigma_v$ | Variance of the monetary policy shock          |   1   |
|$\phi_\pi$ | Central Bank's inflation reaction parameter    |  1.5  |
|$\phi_y$   | Central Bank's product reaction parameter      | 0.5/4 |
|  $\rho$   | Monetary Policy Shock Autoregressive parameter |  0.5  |

We also have $\alpha$, the Cobb Douglas production parameter, is set to 1/3 and is not obtained via the Markov Chain Monte Carlo. We show the densities for the estimated parameters bellow:

```julia

using JLD, StatsPlots

results = load("/home/danielc/Documentos/dsge-hmc_V2/values.jld")
tab = results["samples"]

bet = 0.99
sig = 1
phi = 1
alfa = 1/3
epsilon = 6
theta = 2/3
phi_pi = 1.5
phi_y = 0.5/4
rho_v = 0.5
s2 = 1

to_unit(x) = 1/(1+exp(-x))
to_positive(x) = exp(x)
to_one_inf(x) = exp(x) + 1

true_pars = [alfa,bet,epsilon,theta, sig, s2, phi,phi_pi, phi_y,rho_v]

StatsPlots.density(to_unit.(tab[1:2000,1]), legend = :topleft, label = "Distribution", title = "β")
vline!([true_pars[2]],label = "True Value")
```

```julia
StatsPlots.density(to_positive.(tab[1:2000,2]), label = "Distribution",title = "ϵ")
vline!([true_pars[3]],label = "True Value")
```

```julia
StatsPlots.density(to_unit.(tab[1:2000,3]), label = "Distribution", title = "θ")
vline!([true_pars[4]],label = "True Value")
```

```julia
StatsPlots.density(to_positive.(tab[1:2000,4]), label = "Distribution",title = "σ")
vline!([true_pars[5]],label = "True Value")
```

```julia
StatsPlots.density(to_positive.(tab[1:2000,5]), label = "Distribution", title = "σ_v")
vline!([true_pars[6]],label = "True Value")
```

```julia
StatsPlots.density(to_positive.(tab[1:2000,6]), label = "Distribution",title = "ϕ")
vline!([true_pars[7]],label = "True Value")
```

```julia
StatsPlots.density(to_one_inf.(tab[1:2000,7]), label = "Distribution", title = "ϕ_π")
vline!([true_pars[8]],label = "True Value")
```

```julia
StatsPlots.density(to_positive.(tab[1:2000,8]), label = "Distribution", title = "ϕ_y")
vline!([true_pars[9]],label = "True Value")
```

```julia
StatsPlots.density(to_unit.(tab[1:2000,9]), label = "Distribution", title = "ρ")
vline!([true_pars[10]],label = "True Value")
```

Unfortunately, due to the fact that each iteration requires a number of calls of the function to generate the next sample, the process is slow: it took almost 8 hours in an i5-3210M and 6GB of RAM. The good news is that it scales better than RWMH for large parameter space so for large models the estimation the price of evaluating the function and the derivative for 10 parameters is not much smaller than the price for larger parameters.

[^1]: By analytically I mean not using finite differences or automatic differentiation. Since we depend on a large number of kronecker products, we implement the derivative as functions in Julia

[^2]: Which depends on time since it depends on the estimated state, that depends on time.

[^3]: For now

# Appendix I: Derivates for the Kalman Filter

Lets repeat the space state structure:

$$
y_t = Gx_t + u_t\\
x_{t+1} = Ax_t + v_t
$$

And recall that $u_t \sim N(0,R)$ and $v_t \sim N(0,Q)$. The equations used in the Kalman Filter are listed bellow:

$$
\eta_t = y_t -G\hat{x}_{t|t-1} \quad (i) \\
B_t = GP_{t|t-1}G^{\prime} + R \quad (ii) \\
K_t = AP_{t|t-1}G^{\prime}B_t^{-1} \quad (iii) \\
P_{t+1|t} = AP_{t|t-1}A^{\prime} - K_tB_tK_t^{\prime} + Q \quad (iv)\\
\hat{x}_{t+1|t} = A\hat{x}_{t|t-1} + K_t\eta_t \quad (v)\\
$$

Equation (i) is the error from observing the data now based on the state updated to the last data. Equation (ii) is the error of the observables. Equation (iii) is the Kalman gain. Equation (iv) is the variance of the state tomorrow conditional on the data from today. Equation (v) tells us how we update the state to incorporate the dynamic and the new information from the state.

We clearly need $d\eta_t/d\theta$ and $dB_t/d\theta$, which show up in the loglikelihood. For the rest of this appendix we will drop the $d\theta$ on the denominator. We will use the vec trick:

$$
vec(ABC) = (C^{\prime} \otimes A) vec(B)
$$

And $\otimes$ is the kronecker product. Then, lets differentiate in order. I will use the vec trick and the fact that neither $G$ nor $R$ depends on the parameter vector for most models. I will just use $\nabla$ when we have a gradient. Then we have, for (i):

$$
\nabla \eta_t = -G d\hat{x}_{t|t-1}
$$

Now for (ii):

$$
dB_t = GdP_{t|t-1}G^{\prime}\\
dvec(B_t) = (G \otimes G) dvec(P_{t|t-1})
$$

Now equation (iii):

$$
dK_t = dAP_{t|t-1}G^{\prime} + AdP_{t|t-1}G^{\prime} + dR\\
dvec(K_t) = (GP_{t|t-1} \otimes I) dvec(A) + (G \otimes A) dvec(P_{t|t-1}) + dvec(R)
$$

Now equation (iv):


$$
dP_{t+1|t} = dAP_{t|t-1}A^{\prime} + AdP_{t|t-1}A^{\prime} + AP_{t|t-1}dA^{\prime} - dK_tB_tK_t^{\prime} + \\ - K_tdB_tK_t^{\prime} - K_tB_tdK_t^{\prime} + dQ\\
dvec(P_{t+1|t}) = (AP_{t|t-1}\otimes I) dvec(A) + (A \otimes A) dvec(P_{t|t-1}) + \\ + (I \otimes AP_{t|t-1}) dvec(A^{\prime}) - (K_t B_t \otimes I) dvec(K_t) - (K_t \otimes K_t) dvec(B_t) - (I \otimes K_t B_t) dvec(K_t^{\prime}) + dvec(Q)
$$

Finally, equation (v):

\hat{x}_{t+1|t} = A\hat{x}_{t|t-1} + K_t\eta_t

$$
d\hat{x}_{t+1|t} = dA \hat{x}_{t|t-1} + A d\hat{x}_{t|t-1} + dK_t \eta_t + K_t d\eta_t \\
d\hat{x}_{t+1|t} = (\hat{x}_{t|t-1}^{\prime} \otimes I)dvec(A) + Ad\hat{x}_{t|t-1} + (\eta_t^{\prime} \otimes I) dvec(K_t) + K_t d\eta_t\\
$$

Notice that we need the previous value of the derivative for the derivative of equations (iv) and (v).

# Appendix II: file structure

Folders:

* Imgs: Alll images are saved here
* Reports: All reports are saved here
* old: From the previous attempts at this

Files:

Main files:

* HMC.jl: The main file for estimation, depends on every other file on the folder
* renormalization\_manual.jl: Reparametrization of the posterior so the parameters belong to the whole real line
* llh\_diff.jl: The loglikelihood and its derivative
* priors.jl: The priors for all parameters
* diff\_kalman.jl: The functions that calculate the derivative of the Kalman Filter. It defines a structure to hold what the Kalman filter needs in terms of derivative and functions to compute each derivative that is needed.
* diffs\_estrut\_v3.jl: DIfferentiates the structural model, based on Iskrev (2007). It also holds the matrices needed for the model, which is not a good pratice but is not hard to change it.
* gensys.jl: Our implementation of gensys, described in Sims (2002)   
* aux_matrix.jl: Implementations of duplication and commutation matrices
* simulation.jl: Uses gensys and an input model to generate data from that model.

Tests files:

* test.jl: Tests most of the functions to make sure there are no bugs on them and they return reasonable values
* diff\_tests.jl: Extensive test of the analytical derivatives, compared against finite difference derivative. They do not need to be equal, but should be "close"

Auxiliar files:

* galis\_example.jl and gali\_bayesian.jl: Sets up the matrices for Gali (2008) model example and the parameter values, as well as the guess and verify solutions available in the book so we can compare the IRFs with the IRF from gensys
* autocorrelation.jl: Cheap and quick implementation of a function to calculate autocorrelation

Data cleaning:

* tests\_pos\_mcmc.jl: Just calculates the autocorrelation and the effective sample size for now, in the future will include more assesments of the simulation.

# Bibliography
