---
title: A Julia Implementation of Gensys
author: Gilberto Boaretto & Daniel Coutinho
date: October, 2019
options:
  md2tex:
    out_path: output/gensys.tex
  md2html:
      out_path: output/gensys.html
  out_path: output/gensys.md
---

# Introduction

This document is a white paper on our implementation of gensys, the solution method developed by Sims (2002). Our implementation follows closely Sims's paper and the description in Miao (2014). In general, one takes a model written as:

$$\Gamma_0 y_t = \Gamma_1 y_{t-1} + \Psi \varepsilon_t + \Pi \eta_t$$

In which $y_t$ are the variables today, $\varepsilon_t$ is a vector of exogenous shocks and $\eta_t$ is a vector of endogenous errors, i.e. expectational errors.

 Sims's method allows for a $\Gamma_0$ that is singular. Instead of determining some variables as _jump_ and others as _predetermined_, as Blanchard and Khan (1980) or Klein (2000), Sims allows it to be done endogenously by the algorithm.

Sims's method has another remarkable difference from the previous methods: instead of "root counting" - checking that the number of eigenvalues larger than 1 equals the number of _jump_ variables and the number of eigenvalues smaller than 1 equals the number of _predetermined_ variables - Sims's algorithm relies on ensuring that the expectational errors are linear combinations of the exogenous shocks. We will go into more details in the next sections.

We implement this in Julia. Julia is a modern programming language focused on scientific computing, trying to balance speed and ease to use. Julia has reached maturity only recently. However, there is a large number of packages written for Julia. Thomas Sargent is probably the most famous economist with ties with Julia, dedicating a whole section of his website to the language and how-to solve standard models in economics using Julia - although this does not include large scale DSGEs. [The Federal Reserve Bank of New York (FED-NY) implemented their DSGE on Julia.](https://juliacomputing.com/case-studies/ny-fed.html) By their own account, this sped up estimation ten fold and solving the model was eleven times faster. Given the time it takes to estimate even a medium DSGE model, these gains are not negligibles. Moreover, Julia has a large numerical linear algebra library that comes out of the box (you don't need to install anything besides Julia). This is valuable given that we will work with _linear_ models.

This white paper is divided as follows: in the next section, we briefly discuss two methods of numerical linear algebra that will be used in our implementation: the Schur Decomposition (also called the QZ decomposition) and the Singular Value Decomposition (SVD); section three concisely presents the algorithm and our implementation; section four presents a small example from Gali (2008), which has analytical solution - this allows us to make sure that our implementation is sound; section five discusses the interpreter; the last section concludes.

# Numerical Linear Algebra

We will use two decompositions of matrices: the Schur Decomposition and the Singular Value Decomposition (SVD). Both are closely related to the more well known eigenvalue decomposition:

$$A = P^{-1}\Lambda P$$

In which $\Lambda$ is a diagonal matrix. In general, $P$ is not orthonormal. The eigenvectors matrix, $P$, is orthonormal, i.e. $P^{-1} = P'$ in some cases, for example, if A is normal (Symmetric matrices are normal).

The Schur Decomposition tries to find a decomposition	of the same form of the eigenvalue decomposition, but instead of imposing that $\Lambda$ is diagonal, we now want that P$ be orthonormal. So the Schur Decomposition then finds:

$$A = Q'SQ$$

Remarkably $S$ is always an upper triangular matrix.

We will be actually interested in the Generalized Schur Decomposition. To understand this one, lets first understand what is the generalized eigenvalue. In the usual eigenvalue problem, we want to find a vector $\mathbf{x}$ and a value $\lambda$ such that:

$$Ax = \lambda x$$

Rewritting this in matricial terms, we want:

$$(A - \lambda I)x = 0$$

In which $I$ stands for the identity matrix. Now, what if we want to solve the following problem:

$$Ax = \lambda{}Bx$$

We can find matrices such that $B = P^{-1}P$ and $A = P^{-1}\Lambda{}P$, in which $\Lambda$ is a diagonal matrix.

The idea for the Generalized Schur Decomposition will be the same, but now we want to find matrices $Q,Z,S,T$ such that:

1. $Q$ and $Z$ are orthonormal
2. $S$ and $T$ are upper triangular
3. $A = QSZ$
4. $B = QTZ$
5. $\forall i$ $S_{ii}, T_{ii}$ are not zero
6. The pairs $(S_{ii}, T_{ii})$ can be arranged in any order

Item 6 will be particularly useful in what follows

The SVD also follows the idea from the eigenvalue problem. Now the idea is to decompose A as $USV'$, in which S is a diagonal matrix and both $U$ and $V$ are orthonormal.


# Gensys: algebra

In this section we describe the \cites{sims2002} method for solution of linear rational expectations models -- the \texttt{gensys} solver. Following \citet{miao2014}, the start point is the model with demeaned variables given by
\begin{equation} \label{model}
    \Gamma_0 y_t = \Gamma_1 y_{t-1} + \Psi \varepsilon_t + \Pi \eta_t, \quad \mathbb{E}_{t}\varepsilon_{t+1} = 0,\quad \mathbb{E}_t \eta_{t+1}=0,
\end{equation}
where $y_t$ is an $n \times 1$ random vector, $\varepsilon_t$ is an $n_z \times 1$ vector of exogenous random shocks, $\eta_t$ is an $m \times 1$ vector of endogenously determined expectation errors, $\Gamma_0$ and $\Gamma_1$ are $n \times n$ matrices, $\Psi$ is an $n \times m$ matrix, and $\Pi$ is an $n \times m$ matrix.

Employing the generalized Schur decomposition to $(\Gamma_0,\Gamma_1)$ we obtain $Q\Gamma_0Z = S$ and $Q\Gamma_1Z=T$, where $Q$ and $Z$ are $n \times n$ unitary complex matrices, and $S$ and $T$ are $n \times n$ upper triangular matrices. Now we order $S$ and $T$ in order to we have the first $n_s$ generalized eigenvalues followed by the remaining $n_u$ unstable generalized eigenvalues and we define the partition
\begin{equation} \label{partition}
    y_t^* = Z^H y_t = \begin{bmatrix} Z^H_1 \\ Z^H_2 \end{bmatrix} y_t = \begin{bmatrix} s_t \\ u_t \end{bmatrix},
\end{equation}
where $s_t$ is an $n_s \times 1$ vector and $u_t$ is an $n_u \times 1$ vector.

Premultiplying \eqref{model} by $Q$ we obtain
\begin{align*}
    Q \Gamma_0 y_t &= Q \Gamma_1 y_{t-1} + Q(\Psi \varepsilon_t + \Pi \eta_t)\\
    Q Q^H S Z^H y_t &= Q Q^H T Z^H y_{t-1} + Q(\Psi \varepsilon_t + \Pi \eta_t)\\
    S y_t^* &= T y_{t-1}^* + Q(\Psi \varepsilon_t + \Pi \eta_t)
\end{align*}
and given the partition in \eqref{partition} we can rewrite
\begin{equation} \label{system}
     \begin{bmatrix} S_{11} & S_{12} \\ 0 & S_{22} \end{bmatrix} \begin{bmatrix} s_t \\ u_t \end{bmatrix} = \begin{bmatrix} T_{11} & T_{12} \\ 0 & T_{22} \end{bmatrix} \begin{bmatrix} s_{t-1} \\ u_{t-1}\end{bmatrix} + \begin{bmatrix} Q_1 \\ Q_2 \end{bmatrix} (\Psi \varepsilon_t + \Pi \eta_t)
\end{equation}
where $Q_1$ and $Q_2$ are the first $n_s$ and last $n_u$ rows of $Q$, respectively.

Since $u_t$ corresponds to unstable generalized eigenvalues, we solve it forward:
\begin{gather*}
    S_{22} u_t = T_{22} u_{t-1} + Q_2 (\Psi \varepsilon_t + \Pi \eta_t) \\
    - T_{22} u_{t-1} = - S_{22} u_t + Q_2 (\Psi \varepsilon_t + \Pi \eta_t) \quad \Rightarrow \quad - T_{22} u_{t} = - S_{22} u_{t+1} + Q_2 (\Psi \varepsilon_{t+1} + \Pi \eta_{t+1}) \\
    u_{t} = T_{22}^{-1} S_{22} u_{t+1} - T_{22}^{-1} Q_2 (\Psi \varepsilon_{t+1} + \Pi \eta_{t+1})
\end{gather*}
since $u_{t+1} = T_{22}^{-1} S_{22} u_{t+2} - T_{22}^{-1} Q_2 (\Psi \varepsilon_{t+2} + \Pi \eta_{t+2})$ we have that
\begin{align*}
    u_{t} &= T_{22}^{-1} S_{22} \Big( T_{22}^{-1} S_{22} u_{t+2} - T_{22}^{-1} Q_2 (\Psi \varepsilon_{t+2} + \Pi \eta_{t+2}) \Big) - T_{22}^{-1} Q_2 (\Psi \varepsilon_{t+1} + \Pi \eta_{t+1}) \\
    &= (T_{22}^{-1} S_{22})^2 u_{t+2} - T_{22}^{-1} S_{22} T_{22}^{-1} Q_2 (\Psi \varepsilon_{t+2} + \Pi \eta_{t+2}) - T_{22}^{-1} Q_2 (\Psi \varepsilon_{t+1} + \Pi \eta_{t+1}) \\
    &= (T_{22}^{-1} S_{22})^2 u_{t+2} - \sum_{j=1}^{2-1} (T_{22}^{-1} S_{22})^j T_{22}^{-1} Q_2 (\Psi \varepsilon_{t+j} + \Pi \eta_{t+j})\\
    & \text{} \quad \vdots \\
    &= \cancelto{0}{ \lim_{j \rightarrow \infty} (T_{22}^{-1} S_{22})^j u_{t+j} } \quad - \sum_{j=1}^\infty (T_{22}^{-1} S_{22})^{j-1} T_{22}^{-1} Q_2 (\Psi \varepsilon_{t+j} + \Pi \eta_{t+j}),
\end{align*}
that is,
\red{
\begin{equation*}
    u_t = - \sum_{j=1}^\infty (T_{22}^{-1} S_{22})^{j-1} T_{22}^{-1} Q_2 (\Psi \varepsilon_{t+j} + \Pi \eta_{t+j})
\end{equation*}
}

Taking expectations,
\begin{equation*}
    \mathbb{E}_t u_t = u_t = 0, \quad \forall t \qquad \Rightarrow \qquad u_t = Z_2^H y_t = 0, \quad \forall t \geq 0.%,
\end{equation*}
%which gives condition to determine the initial value $y_0$.

Combining \eqref{system} and $u_t=0$ we obtain
\begin{equation}
    Q_2 (\Psi \varepsilon_t + \Pi \eta_t) = 0
\end{equation}
and our goal is to solve for $\eta_t$ in terms of $\varepsilon_t$. Thus, for existence and uniqueness of solution of \eqref{model}, following two assumptions are necessary and sufficient:

\vspace{0.3cm}

\noindent \textbf{Assumption 1 (for existence).} There exists $m \times n_z$ matrix $\Lambda $ such that $Q_2 \Psi = Q_2 \Pi \Lambda$.

\vspace{0.15cm}

\noindent \textbf{Assumption 2 (for uniqueness).} There exists $n_s \times n_u$ matrix $\Xi$ such that $Q_1 \Pi = \Xi Q_2 \Pi$.

\vspace{0.3cm}

Premultiplying \eqref{system} by $\begin{bmatrix} 0 & -\Xi \end{bmatrix}$:
\begin{align*}
    \begin{bmatrix} 0 & -\Xi \end{bmatrix} \begin{bmatrix} S_{11} & S_{12} \\ 0 & S_{22} \end{bmatrix} \begin{bmatrix} s_t \\ u_t \end{bmatrix} &= \begin{bmatrix} 0 & -\Xi \end{bmatrix} \begin{bmatrix} T_{11} & T_{12} \\ 0 & T_{22} \end{bmatrix} \begin{bmatrix} s_{t-1} \\ u_{t-1} \end{bmatrix} + \begin{bmatrix} 0 & -\Xi \end{bmatrix} \begin{bmatrix} Q_1 \\ Q_2 \end{bmatrix} (\Psi \varepsilon_t + \Pi \eta_t) \\
    \begin{bmatrix} S_{11} & S_{12}-\Xi S_{22} \end{bmatrix} \begin{bmatrix} s_t \\ u_t \end{bmatrix} &= \begin{bmatrix} T_{11} & T_{12}-\xi T_{22} \end{bmatrix} \begin{bmatrix} s_{t-1} \\ u_{t-1} \end{bmatrix} + (Q_1 - \Xi Q_2)(\Psi \varepsilon_t + \Pi \eta_t)
\end{align*}

Extending the system to the result $u_t = 0$:
\begin{equation} \label{system2}
    \begin{bmatrix} S_{11} & S_{12}-\Xi S_{22} \\ 0 & I \end{bmatrix} \begin{bmatrix} s_t \\ u_t \end{bmatrix} = \begin{bmatrix} T_{11} & T_{12}-\xi T_{22} \\ 0 & 0 \end{bmatrix} \begin{bmatrix} s_{t-1} \\ u_{t-1} \end{bmatrix} + \begin{bmatrix} Q_1 - \Xi Q_2 \\ 0 \end{bmatrix} (\Psi \varepsilon_t + \Pi \eta_t)
\end{equation}

Employing assumption 2 we have that
\begin{align*}
    (Q_1 - \Xi Q_2)(\Psi \varepsilon_t + \Pi \eta_t) &= Q_1\Psi\varepsilon_t + Q_1\Pi\eta_t - \Xi Q_2\Psi\varepsilon_t - \Xi Q_2\Pi\eta_t \\
    &= Q_1\Psi\varepsilon_t + \cancel{\Xi Q_2\Pi\eta_t} - \Xi Q_2\Psi\varepsilon_t - \cancel{\Xi Q_2\Pi\eta_t} \\
    &= Q_1\Psi\varepsilon_t - \Xi Q_2\Psi\varepsilon_t \\
    &= (Q_1 - \Xi Q_2)\Psi\varepsilon_t
\end{align*}

Thus the system \eqref{system2} take the form
\begin{equation*}
    \begin{bmatrix} S_{11} & S_{12}-\Xi S_{22} \\ 0 & I \end{bmatrix} \begin{bmatrix} s_t \\ u_t \end{bmatrix} = \begin{bmatrix} T_{11} & T_{12}-\xi T_{22} \\ 0 & 0 \end{bmatrix} \begin{bmatrix} s_{t-1} \\ u_{t-1} \end{bmatrix} + \begin{bmatrix} Q_1 - \Xi Q_2 \\ 0 \end{bmatrix} \Psi \varepsilon_t
\end{equation*}

Since $y_t^* = \begin{bmatrix} s_t \\ u_t \end{bmatrix} = Z^H y_t$, we can rewrite
\begin{gather*}
    \begin{bmatrix} S_{11} & S_{12}-\Xi S_{22} \\ 0 & I \end{bmatrix} Z^H y_t = \begin{bmatrix} T_{11} & T_{12}-\xi T_{22} \\ 0 & 0 \end{bmatrix} Z^H y_{t-1} + \begin{bmatrix} Q_1 - \Xi Q_2 \\ 0 \end{bmatrix} \Psi \varepsilon_t \\
    \Rightarrow \quad y_t = \Bigg( \begin{bmatrix} S_{11} & S_{12}-\Xi S_{22} \\ 0 & I \end{bmatrix} Z^H \Bigg)^{-1} \begin{bmatrix} T_{11} & T_{12}-\xi T_{22} \\ 0 & 0 \end{bmatrix} Z^H y_{t-1} + \qquad \qquad \qquad \qquad \qquad \qquad \qquad \\ \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \Bigg( \begin{bmatrix} S_{11} & S_{12}-\Xi S_{22} \\ 0 & I \end{bmatrix} Z^H \Bigg)^{-1} \begin{bmatrix} Q_1 - \Xi Q_2 \\ 0 \end{bmatrix} \Psi \varepsilon_t
\end{gather*}

and the solution of \eqref{model} is given by
\begin{equation*}
    y_t = \Theta_1 y_{t-1} + \Theta_2 \varepsilon_t
\end{equation*}
where
\begin{align*}
    \Theta_1 &= Z \begin{bmatrix} S_{11} & S_{12}-\Xi S_{22} \\ 0 & I \end{bmatrix}^{-1} \begin{bmatrix} T_{11} & T_{12}-\xi T_{22} \\ 0 & 0 \end{bmatrix} Z^H \quad \text{and} \\
    \Theta_2 &= Z \begin{bmatrix} S_{11} & S_{12}-\Xi S_{22} \\ 0 & I \end{bmatrix}^{-1} \begin{bmatrix} Q_1 - \Xi Q_2 \\ 0 \end{bmatrix} Z^H.
\end{align*}

To implement the solution, we need to apply the singular value decomposition (SVD) to $Q_2\Pi$ since its rows in general are linearly dependent. Thus,
\begin{equation}
    Q_2\Pi = U D V^H = \begin{bmatrix} U_1 & U_2 \end{bmatrix} \begin{bmatrix} D_{11} & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} V_1^H \\ V_2^H \end{bmatrix} = U_1 D_{11} V_1^H
\end{equation}
where $D_{11}$ is a non-singular $r \times r$ diagonal matrices, and $U$ and $V$ are $n_u \times n_u$ and $m \times m$ unitary matrices, respectively. Thus $U_1$, $D_{11} $ and $V_1^H$ are $n_u \times r$, $r \times r$ and $r \times m$ matrices, respectively.

If $m=r$, that is, if the number of expectation errors is equals to the number of nonzero singular values of $Q_2\Pi$, then $V_2 = 0$ and from Assumption 1, $Q_2 \Pi \Lambda = Q_2 \Pi$, thus
\begin{equation*}
    U_1 D_{11} V_1^H \Lambda = Q_2 \Pi \quad \Rightarrow \quad \Lambda = V_1 D{11}^{-1} U_1^H Q_2 \Pi,
\end{equation*}
and from Assumption 2, $\Xi Q_2 \Pi = Q_1 \Pi$, thus
\begin{equation*}
    \Xi U_1 D_{11} V_1^H = Q_1 \Pi \quad \Rightarrow \quad \Xi = Q_1 \Pi V_1 D_{11}^{-1} U_1^H.
\end{equation*}

Notice that the last expression can be obtained from the Assumption 1 under $m=r$. Hence, if $m=r$, Assumption 1 alone is able to guarantee existence and uniqueness of solution for \eqref{model}.

If $m>r$, that is, if the number of expectation errors is greater than the number of nonzero singular values of $Q_2\Pi$, then there exists a infinity of stables solutions (due to so-called sunspot shocks). For more detail see \citet{miao2014} pp. 47-48.


# An example

As a proof of concept, lets compare our code with the model of Gali (2008), chapter 3 - this is the simplest standard New Keynesian Model. Lets load first our code for Gensys:

```julia

using Plots

include(string(pwd(), "/src/gensys.jl"))
```

We also loaded the library Plots. We will calibrate exactly as Gali does, on page 52:

```julia
bet = 0.99
sig = 1
phi = 1
alfa = 1/3
epsilon = 6
theta = 2/3
phi_pi = 1.5
phi_y = 0.5/4
rho_v = 0.5

THETA = (1-alfa)/(1-alfa+alfa*epsilon)
lamb = (1-theta)*(1-bet*theta)/theta*THETA
kappa = lamb*(sig+(phi+alfa)/(1-alfa))
```

We refer the reader to Gali's book for knowing what each parameter means. Next, we need to write the usual model in Sim's format. Let us repeat the three equations of the usual model:

$$
\pi_t = \beta E_t(\pi_{t+1}) + \kappa \tilde{y}_t\\
\tilde{y}_t = -\frac{1}{\sigma} (i_t - E_t(\pi_{t+1})) + E_t(\tilde{y}_{t+1})\\
i_t = \phi_{\pi} \pi_t + \phi_{\tilde{y}} \tilde{y}_t + v_t\\
v_t = \rho_v v_{t-1} + \varepsilon_t^v
$$

Gali's book gives an analytical solution for the impulse response functions for a shock $v_t$. This can be found on page 51. We implement them in Julia as well:

```julia

LAMBDA_v = ((1-bet*rho_v)*(sig*(1-rho_v)+phi_y)+kappa*(phi_pi-rho_v))^(-1)

y_irf(v) = -(1-bet*rho_v)*LAMBDA_v*v
pi_irf(v) = -kappa*LAMBDA_v*v
r_irf(v) = sig*(1-rho_v)*(1-bet*rho_v)*LAMBDA_v*v
i_irf(v) = (sig*(1-rho_v)*(1-bet*rho_v) - rho_v*kappa)*LAMBDA_v*v
v_irf(v,uu) = rho_v*v + uu
```

And we compute a sample IRF for a positive 25 basis shock in the interest rate:


```julia

irfs_true = zeros(15,5)

irfs_true[1,1] = 0

uu = 0.25

for t in 2:15
    irfs_true[t,4] = v_irf(irfs_true[t-1,4],uu)
    irfs_true[t,1] = pi_irf(irfs_true[t,4])
    irfs_true[t,2] = y_irf(irfs_true[t,4])
    irfs_true[t,3] = i_irf(irfs_true[t,4])
    irfs_true[t,5] = r_irf(irfs_true[t,4])
    global uu = 0
end
```

Now lets solve it numerically: the first step is to write $\eta_t^{\pi} = \pi_t - E_{t-1} \pi_t$ and $\eta_t^{\tilde{y}} = \tilde{y}_t - E_{t-1} \tilde{y}_t$. Now, use this to isolate to write the expectations as function of the true value plus the expectational error - $E_t(\pi_{t+1}) = \pi_t + \eta_t^{\pi}$ for example.

This allows us to write the system above as follows:

$$
\pi_{t-1} - \kappa \tilde{y}_{t-1} + \beta \eta_t^{\pi} = \beta \pi_t\\
\sigma\tilde{y}_{t-1} + i_{t-1} + \eta_t^{\pi} + \sigma \eta_t^{\tilde{y}} = \pi_t + \sigma \tilde{y}_t\\
i_{t-1} - \phi_{\pi}\pi_{t-1} - \phi_{\tilde{y}} \tilde{y}_{t-1} - v_{t-1} = 0\\
v_t = \rho v_{t-1} + \varepsilon^v_t
$$

And it is straight foward to write the matrices $\Gamma_0$, $\Gamma_1$, $\Psi$, $\Pi$, as follows:

```julia

GAMMA_0 = [bet    0     0  0;
           1      sig   0  0;
           0      0     0  0;
           0      0     0  1]

GAMMA_1 = [1      -kappa  0  0;
           0       sig    1  0;
           -phi_pi  -phi_y  1 -1;
           0       0      0  rho_v]

PSI = [0; 0; 0; 1]

PI = [bet  0;
      1    sig;
      0    0;
      0    0]
```

Here are LaTeX versions of the matrices generated automatically with the package _latexify_, from the inputed matrices for Julia:

```julia; results = "tex"

using Latexify

print(string("\$\$\\Gamma_0 = \$\$",latexify(GAMMA_0)))

```

```julia; results = "tex"

print(string("\$\$\\Gamma_1 = \$\$",latexify(GAMMA_1)))

```
```julia; results = "tex"

print(string("\$\$\\Psi = \$\$",latexify(PSI)))

```

```julia; results = "tex"

print(string("\$\$\\Pi = \$\$",latexify(PI)))

```

And we just call the function with this matrices:

```julia
sol = gensys(GAMMA_0,GAMMA_1,PSI,PI)

irfs = irf(sol,15,0.25)
```

Lets compare the IRFs generated analyticaly and by our implementation of Gensys:

```julia

plot(4*irfs[:,1], label = "Inflation Gensys", w =3)
plot!(4*irfs_true[2:15,1], label = "Inflation Analytical", line = :dash, w =3)
title!("Inflation response for a 0.25 shock in interest rate")
```

```julia

plot(irfs[:,2], label = "Output Gap Gensys", w = 3)
plot!(irfs_true[2:15,2], label = "Output Gap Analytical", line = :dash, w = 3)
title!("Output Gap response for a 0.25 shock in interest rate")
```

```julia

plot(4*irfs[:,3], label = "Interest Rate Gensys", w = 3)
plot!(4*irfs_true[2:15,3], label = "Interest Rate Analytical", line = :dash, w = 3)
title!("Nominal Interest response for a 0.25 shock in interest rate")
```

```julia

plot(irfs[:,4], label = "v Gensys", w = 3)
plot!(irfs_true[2:15,4], label = "v Analytical", line = :dash, w = 3)
title!("v response for a 0.25 shock in interest rate")
```

The impulse response are identical between the two methods, which is a strong evidence of the reliability and precision of our implementation of Gensys, Gensys itself and Julia capabilities to handle linear algebra routines.

# Interpreter

While our gensys implementation is sound, it has a downside: one needs to substitute the expectation variables by hand and build the matrices $\Gamma_0$,$\Gamma_1$,$\Psi$,$\Pi$ by hand. While this is feasible and not too painful for a model with 4 equations, it is prone to errors and cumbersome, especially for larger systems. Besides, this is not extremely user friendly. One would like to achieve a somewhat similar capacity to Dynare, in which the reasearcher just inputs the equations in a natural way and the whole system takes care of building the matrices and solving it. We will use the package **DynamicPolynomials**, since our system is always linear (a polynomial). Although the name may be misleading, the packages *was not* made to work with difference equations. One has to use a bit of ingenuity to write the interpreter. We adopt the following convention for the variables:

* `x_e` means the expected value of x
* `x_l` means the lag of x

And Sims's method relies on substituting $E_t(x_{t+1}) = x_{t+1} + \eta_t^{x}$. All one has to do is:

1. Write a program that automatic detects `_e` on variables
2. Automatically create the variables $x_{t+1}$ and $\eta_t^x$
3. Substitute every expected variable as its true future value plus expectation error.
4. Recovers the coefficient associated with each variables and puts them in the right matrix

While we won't bother the reader with all the details, the rough outline of how we do each is step is as following:

1. Regular Expressions can do it just fine
2. Every program in Julia starts its life as a string, so concatenation of strings allow us to do the second step, with some `eval()`.
3. and 4. Are mainly delt by the functions in the **DynamicPolynomials** package and some loops.

The model needs to be written into a Dictionary, and unlike Dynare, all the variables have to be on the same side of the equation. So the model by Gali could be written as follows. One needs first to declare the variables so **DynamicPolynomials** can deal with them, using the macro `@polyvar`:

```julia

 include(string(pwd(), "/src/interpreter.jl"))

 @polyvar pi pi_e y y_e i v v_l ep
```

Lets write the four equations:

```julia

 eq1 = pi - bet*pi_e - kappa*y
 eq2 = y + 1/sig*(i - pi_e) - y_e
 eq3 = i - phi_pi*pi - phi_y*y - v_l
 eq4 = v - rho_v*v_l - ep
```

And now the Dictionary with all the equations. We can choose any key we like for the dictionary and to make it easy to anyone to understand what each equation is, we will give them their usual names:

```julia

model = Dict("IS"=>eq2,"Phillips"=>eq1,"Taylor"=>eq3,"Ar Shock"=>eq4)
```

The interpreter itself takes the model and which variables represent shocks. Everything else is dealt internally:

```julia

modelgensys = model2gensys(model,[ep])
```

The object `modelgensys` contains $\Gamma_0$,$\Gamma_1$,$\Psi$ and $\Pi$. We wrote a `gensys` function that takes the whole model and automatically computes the solutions - this is only possible because Julia allows for multiple dispatch (the same function can do different things, deppending on which type of object is passed to them):

```julia

sol = gensys(modelgensys)
```

The IRFs should match the analytical results:

```julia

 fris = irf(sol,15,0.25)

 plot(4*fris[:,1], label = "Inflation Gensys", w =3)
 plot!(4*irfs_true[2:15,1], label = "Inflation Analytical", line = :dash, w =3)
 title!("Inflation response for a 0.25 shock in interest rate")
```

```julia

plot(fris[:,2], label = "Output Gap Gensys", w = 3)
plot!(irfs_true[2:15,2], label = "Output Gap Analytical", line = :dash, w = 3)
title!("Output Gap response for a 0.25 shock in interest rate")
```

```julia

plot(4*fris[:,4], label = "Interest Rate Gensys", w = 3)
plot!(4*irfs_true[2:15,3], label = "Interest Rate Analytical", line = :dash, w = 3)
title!("Nominal Interest response for a 0.25 shock in interest rate")
```

```julia

plot(fris[:,3], label = "v Gensys", w = 3)
plot!(irfs_true[2:15,4], label = "v Analytical", line = :dash, w = 3)
title!("v response for a 0.25 shock in interest rate")
```
