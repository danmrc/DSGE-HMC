<!DOCTYPE html>
<HTML lang = "en">
<HEAD>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>DSGE in Julia</title>
  

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>

  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  
<style>
pre.hljl {
    border: 1px solid #ccc;
    margin: 5px;
    padding: 5px;
    overflow-x: auto;
    color: rgb(68,68,68); background-color: rgb(251,251,251); }
pre.hljl > span.hljl-t { }
pre.hljl > span.hljl-w { }
pre.hljl > span.hljl-e { }
pre.hljl > span.hljl-eB { }
pre.hljl > span.hljl-o { }
pre.hljl > span.hljl-k { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kc { color: rgb(59,151,46); font-style: italic; }
pre.hljl > span.hljl-kd { color: rgb(214,102,97); font-style: italic; }
pre.hljl > span.hljl-kn { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kp { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kr { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kt { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-n { }
pre.hljl > span.hljl-na { }
pre.hljl > span.hljl-nb { }
pre.hljl > span.hljl-nbp { }
pre.hljl > span.hljl-nc { }
pre.hljl > span.hljl-ncB { }
pre.hljl > span.hljl-nd { color: rgb(214,102,97); }
pre.hljl > span.hljl-ne { }
pre.hljl > span.hljl-neB { }
pre.hljl > span.hljl-nf { color: rgb(66,102,213); }
pre.hljl > span.hljl-nfm { color: rgb(66,102,213); }
pre.hljl > span.hljl-np { }
pre.hljl > span.hljl-nl { }
pre.hljl > span.hljl-nn { }
pre.hljl > span.hljl-no { }
pre.hljl > span.hljl-nt { }
pre.hljl > span.hljl-nv { }
pre.hljl > span.hljl-nvc { }
pre.hljl > span.hljl-nvg { }
pre.hljl > span.hljl-nvi { }
pre.hljl > span.hljl-nvm { }
pre.hljl > span.hljl-l { }
pre.hljl > span.hljl-ld { color: rgb(148,91,176); font-style: italic; }
pre.hljl > span.hljl-s { color: rgb(201,61,57); }
pre.hljl > span.hljl-sa { color: rgb(201,61,57); }
pre.hljl > span.hljl-sb { color: rgb(201,61,57); }
pre.hljl > span.hljl-sc { color: rgb(201,61,57); }
pre.hljl > span.hljl-sd { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdB { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdC { color: rgb(201,61,57); }
pre.hljl > span.hljl-se { color: rgb(59,151,46); }
pre.hljl > span.hljl-sh { color: rgb(201,61,57); }
pre.hljl > span.hljl-si { }
pre.hljl > span.hljl-so { color: rgb(201,61,57); }
pre.hljl > span.hljl-sr { color: rgb(201,61,57); }
pre.hljl > span.hljl-ss { color: rgb(201,61,57); }
pre.hljl > span.hljl-ssB { color: rgb(201,61,57); }
pre.hljl > span.hljl-nB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nbB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nfB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nh { color: rgb(59,151,46); }
pre.hljl > span.hljl-ni { color: rgb(59,151,46); }
pre.hljl > span.hljl-nil { color: rgb(59,151,46); }
pre.hljl > span.hljl-noB { color: rgb(59,151,46); }
pre.hljl > span.hljl-oB { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-ow { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-p { }
pre.hljl > span.hljl-c { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-ch { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cm { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cp { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cpB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cs { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-csB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-g { }
pre.hljl > span.hljl-gd { }
pre.hljl > span.hljl-ge { }
pre.hljl > span.hljl-geB { }
pre.hljl > span.hljl-gh { }
pre.hljl > span.hljl-gi { }
pre.hljl > span.hljl-go { }
pre.hljl > span.hljl-gp { }
pre.hljl > span.hljl-gs { }
pre.hljl > span.hljl-gsB { }
pre.hljl > span.hljl-gt { }
</style>



  <style type="text/css">
  @font-face {
  font-style: normal;
  font-weight: 300;
}
@font-face {
  font-style: normal;
  font-weight: 400;
}
@font-face {
  font-style: normal;
  font-weight: 600;
}
html {
  font-family: sans-serif; /* 1 */
  -ms-text-size-adjust: 100%; /* 2 */
  -webkit-text-size-adjust: 100%; /* 2 */
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block; /* 1 */
  vertical-align: baseline; /* 2 */
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit; /* 1 */
  font: inherit; /* 2 */
  margin: 0; /* 3 */
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button; /* 2 */
  cursor: pointer; /* 3 */
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box; /* 1 */
  padding: 0; /* 2 */
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield; /* 1 */
  -moz-box-sizing: content-box;
  -webkit-box-sizing: content-box; /* 2 */
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0; /* 1 */
  padding: 0; /* 2 */
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  font-family: monospace, monospace;
  font-size : 0.8em;
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
thead th {
    border-bottom: 1px solid black;
    background-color: white;
}
tr:nth-child(odd){
  background-color: rgb(248,248,248);
}


/*
* Skeleton V2.0.4
* Copyright 2014, Dave Gamache
* www.getskeleton.com
* Free to use under the MIT license.
* http://www.opensource.org/licenses/mit-license.php
* 12/29/2014
*/
.container {
  position: relative;
  width: 100%;
  max-width: 960px;
  margin: 0 auto;
  padding: 0 20px;
  box-sizing: border-box; }
.column,
.columns {
  width: 100%;
  float: left;
  box-sizing: border-box; }
@media (min-width: 400px) {
  .container {
    width: 85%;
    padding: 0; }
}
@media (min-width: 550px) {
  .container {
    width: 80%; }
  .column,
  .columns {
    margin-left: 4%; }
  .column:first-child,
  .columns:first-child {
    margin-left: 0; }

  .one.column,
  .one.columns                    { width: 4.66666666667%; }
  .two.columns                    { width: 13.3333333333%; }
  .three.columns                  { width: 22%;            }
  .four.columns                   { width: 30.6666666667%; }
  .five.columns                   { width: 39.3333333333%; }
  .six.columns                    { width: 48%;            }
  .seven.columns                  { width: 56.6666666667%; }
  .eight.columns                  { width: 65.3333333333%; }
  .nine.columns                   { width: 74.0%;          }
  .ten.columns                    { width: 82.6666666667%; }
  .eleven.columns                 { width: 91.3333333333%; }
  .twelve.columns                 { width: 100%; margin-left: 0; }

  .one-third.column               { width: 30.6666666667%; }
  .two-thirds.column              { width: 65.3333333333%; }

  .one-half.column                { width: 48%; }

  /* Offsets */
  .offset-by-one.column,
  .offset-by-one.columns          { margin-left: 8.66666666667%; }
  .offset-by-two.column,
  .offset-by-two.columns          { margin-left: 17.3333333333%; }
  .offset-by-three.column,
  .offset-by-three.columns        { margin-left: 26%;            }
  .offset-by-four.column,
  .offset-by-four.columns         { margin-left: 34.6666666667%; }
  .offset-by-five.column,
  .offset-by-five.columns         { margin-left: 43.3333333333%; }
  .offset-by-six.column,
  .offset-by-six.columns          { margin-left: 52%;            }
  .offset-by-seven.column,
  .offset-by-seven.columns        { margin-left: 60.6666666667%; }
  .offset-by-eight.column,
  .offset-by-eight.columns        { margin-left: 69.3333333333%; }
  .offset-by-nine.column,
  .offset-by-nine.columns         { margin-left: 78.0%;          }
  .offset-by-ten.column,
  .offset-by-ten.columns          { margin-left: 86.6666666667%; }
  .offset-by-eleven.column,
  .offset-by-eleven.columns       { margin-left: 95.3333333333%; }

  .offset-by-one-third.column,
  .offset-by-one-third.columns    { margin-left: 34.6666666667%; }
  .offset-by-two-thirds.column,
  .offset-by-two-thirds.columns   { margin-left: 69.3333333333%; }

  .offset-by-one-half.column,
  .offset-by-one-half.columns     { margin-left: 52%; }

}
html {
  font-size: 62.5%; }
body {
  font-size: 1.5em; /* currently ems cause chrome bug misinterpreting rems on body element */
  line-height: 1.6;
  font-weight: 400;
  font-family: "Raleway", "HelveticaNeue", "Helvetica Neue", Helvetica, Arial, sans-serif;
  color: #222; }
h1, h2, h3, h4, h5, h6 {
  margin-top: 0;
  margin-bottom: 2rem;
  font-weight: 300; }
h1 { font-size: 3.6rem; line-height: 1.2;  letter-spacing: -.1rem;}
h2 { font-size: 3.4rem; line-height: 1.25; letter-spacing: -.1rem; }
h3 { font-size: 3.2rem; line-height: 1.3;  letter-spacing: -.1rem; }
h4 { font-size: 2.8rem; line-height: 1.35; letter-spacing: -.08rem; }
h5 { font-size: 2.4rem; line-height: 1.5;  letter-spacing: -.05rem; }
h6 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }

p {
  margin-top: 0; }
a {
  color: #1EAEDB; }
a:hover {
  color: #0FA0CE; }
.button,
button,
input[type="submit"],
input[type="reset"],
input[type="button"] {
  display: inline-block;
  height: 38px;
  padding: 0 30px;
  color: #555;
  text-align: center;
  font-size: 11px;
  font-weight: 600;
  line-height: 38px;
  letter-spacing: .1rem;
  text-transform: uppercase;
  text-decoration: none;
  white-space: nowrap;
  background-color: transparent;
  border-radius: 4px;
  border: 1px solid #bbb;
  cursor: pointer;
  box-sizing: border-box; }
.button:hover,
button:hover,
input[type="submit"]:hover,
input[type="reset"]:hover,
input[type="button"]:hover,
.button:focus,
button:focus,
input[type="submit"]:focus,
input[type="reset"]:focus,
input[type="button"]:focus {
  color: #333;
  border-color: #888;
  outline: 0; }
.button.button-primary,
button.button-primary,
input[type="submit"].button-primary,
input[type="reset"].button-primary,
input[type="button"].button-primary {
  color: #FFF;
  background-color: #33C3F0;
  border-color: #33C3F0; }
.button.button-primary:hover,
button.button-primary:hover,
input[type="submit"].button-primary:hover,
input[type="reset"].button-primary:hover,
input[type="button"].button-primary:hover,
.button.button-primary:focus,
button.button-primary:focus,
input[type="submit"].button-primary:focus,
input[type="reset"].button-primary:focus,
input[type="button"].button-primary:focus {
  color: #FFF;
  background-color: #1EAEDB;
  border-color: #1EAEDB; }
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea,
select {
  height: 38px;
  padding: 6px 10px; /* The 6px vertically centers text on FF, ignored by Webkit */
  background-color: #fff;
  border: 1px solid #D1D1D1;
  border-radius: 4px;
  box-shadow: none;
  box-sizing: border-box; }
/* Removes awkward default styles on some inputs for iOS */
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea {
  -webkit-appearance: none;
     -moz-appearance: none;
          appearance: none; }
textarea {
  min-height: 65px;
  padding-top: 6px;
  padding-bottom: 6px; }
input[type="email"]:focus,
input[type="number"]:focus,
input[type="search"]:focus,
input[type="text"]:focus,
input[type="tel"]:focus,
input[type="url"]:focus,
input[type="password"]:focus,
textarea:focus,
select:focus {
  border: 1px solid #33C3F0;
  outline: 0; }
label,
legend {
  display: block;
  margin-bottom: .5rem;
  font-weight: 600; }
fieldset {
  padding: 0;
  border-width: 0; }
input[type="checkbox"],
input[type="radio"] {
  display: inline; }
label > .label-body {
  display: inline-block;
  margin-left: .5rem;
  font-weight: normal; }
ul {
  list-style: circle; }
ol {
  list-style: decimal; }
ul ul,
ul ol,
ol ol,
ol ul {
  margin: 1.5rem 0 1.5rem 3rem;
  font-size: 90%; }
li > p {margin : 0;}
th,
td {
  padding: 12px 15px;
  text-align: left;
  border-bottom: 1px solid #E1E1E1; }
th:first-child,
td:first-child {
  padding-left: 0; }
th:last-child,
td:last-child {
  padding-right: 0; }
button,
.button {
  margin-bottom: 1rem; }
input,
textarea,
select,
fieldset {
  margin-bottom: 1.5rem; }
pre,
blockquote,
dl,
figure,
table,
p,
ul,
ol,
form {
  margin-bottom: 1.0rem; }
.u-full-width {
  width: 100%;
  box-sizing: border-box; }
.u-max-full-width {
  max-width: 100%;
  box-sizing: border-box; }
.u-pull-right {
  float: right; }
.u-pull-left {
  float: left; }
hr {
  margin-top: 3rem;
  margin-bottom: 3.5rem;
  border-width: 0;
  border-top: 1px solid #E1E1E1; }
.container:after,
.row:after,
.u-cf {
  content: "";
  display: table;
  clear: both; }

pre {
  display: block;
  padding: 9.5px;
  margin: 0 0 10px;
  font-size: 13px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre.hljl {
  margin: 0 0 10px;
  display: block;
  background: #f5f5f5;
  border-radius: 4px;
  padding : 5px;
}

pre.output {
  background: #ffffff;
}

pre.code {
  background: #ffffff;
}

pre.julia-error {
  color : red
}

code,
kbd,
pre,
samp {
  font-family: Menlo, Monaco, Consolas, "Courier New", monospace;
  font-size: 13px;
}


@media (min-width: 400px) {}
@media (min-width: 550px) {}
@media (min-width: 750px) {}
@media (min-width: 1000px) {}
@media (min-width: 1200px) {}

h1.title {margin-top : 20px}
img {max-width : 100%}
div.title {text-align: center;}

  </style>



</HEAD>
  <BODY>
    <div class ="container">
      <div class = "row">
        <div class = "col-md-12 twelve columns">

          <div class="title">
            <h1 class="title">DSGE in Julia</h1>
            <h5>Gilberto Boaretto & Daniel Coutinho</h5>
            <h5>December, 2019</h5>
          </div>

          <h1>Introduction</h1>
<p>We implement a solver and estimation procedure for Dynamic Stochastic General Equilibrium &#40;DSGE&#41; models using Julia, a modern programming language focused on scientific computing. We show all of this working for a toy model, from Gali &#40;2008&#41;. This allows us to test throughout every step of the code and show that it is sound. This is a rather large enterprise and the final code relies on a number of packages and files. To help the reader, we give an overview of what we did before dwelling into details:</p>
<ol>
<li><p>We need to take a log linearized DSGE, which depends on expectations of future values, and write it in a form that allow us to take it into the data. <em>Gensys</em>, as described in Sims &#40;2002&#41; allow us to do that and is therefore the first step.</p>
</li>
<li><p>We need to write, given a set of parameters and some data, the likelihood. This is not trivial since the model is a VARMA. The Kalman Filter allows us to recover the fundamental shocks from the set of observables.</p>
</li>
<li><p>The last step is the estimation itself. Due to the large number of parameters and the many restrictions in the parameter space, DSGE are usually estimated by bayesian methods. We follow this approach and implement the Random Walk Metropolis Hasting algorithm to sample from the posterior. We also estimate using a more modern method for Markov Chain Monte Carlo &#40;MCMC&#41;, namely Hamiltonian Monte Carlo &#40;HMC&#41;.</p>
</li>
</ol>
<p>Although there is a large literature on solving and estimating DSGEs, tricks that are important from the computational point of view - many regarding floating point arithmetics - are usually not covered. We discuss a number of them and many &quot;work around&quot; that make algorithms work smoothly.</p>
<p>This paper is composed as follows: the first section is this short introduction; the second one explains the model; the third explains the implementation and its various sub components; the fourth shows the results from a simulation of the toy model and recovering the distribution of the parameters using our algorithm and HMC; the fifth section concludes.</p>
<p>Before we start, let us give a short note on developing something like this: this is a large project, with many pieces that can potentially go wrong for a number of reasons - the first is, of course, human error. We are careful in implementing each step and testing it throughout before adding a new layer. Our approach is to always write one of the parts as a function, test it comparing the results with analytical results or whatever the answer is reasonable. An example of the former is our implementation of gensys, in which we can compare the impulse response functions &#40;IRFs&#41; given by the algorithm with the analytical IRFs; an example of the latter is computing the likelihood over a grid: we would expect that there is a local maximum around the true value of the parameters, fixing all other parameters in their true values.  </p>
<h1>The Model</h1>
<p>Gali &#40;2008&#41; gives a standard three equations New Keynesian DSGE model:</p>
<p class="math">\[
\pi_t = \beta E_t(\pi_{t+1}) + \kappa \tilde{y}_t\\
\tilde{y}_t = -\frac{1}{\sigma} (i_t - E_t(\pi_{t+1})) + E_t(\tilde{y}_{t+1})\\
i_t = \phi_{\pi} \pi_t + \phi_{\tilde{y}} \tilde{y}_t + v_t\\
v_t = \rho_v v_{t-1} + \varepsilon_t^v
\]</p>
<p>We refer the reader to the book for understanding how to go from the optimization problem to this log linear equations and what each parameter means. The calibrated parameters by Gali are the following:</p>


<pre class='hljl'>
<span class='hljl-n'>bet</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.99</span><span class='hljl-t'>
</span><span class='hljl-n'>sig</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-t'>
</span><span class='hljl-n'>phi</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-t'>
</span><span class='hljl-n'>alfa</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>/</span><span class='hljl-ni'>3</span><span class='hljl-t'>
</span><span class='hljl-n'>epsilon</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>6</span><span class='hljl-t'>
</span><span class='hljl-n'>theta</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-oB'>/</span><span class='hljl-ni'>3</span><span class='hljl-t'>
</span><span class='hljl-n'>phi_pi</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.5</span><span class='hljl-t'>
</span><span class='hljl-n'>phi_y</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-oB'>/</span><span class='hljl-ni'>4</span><span class='hljl-t'>
</span><span class='hljl-n'>rho_v</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.5</span><span class='hljl-t'>

</span><span class='hljl-n'>THETA</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>-</span><span class='hljl-n'>alfa</span><span class='hljl-p'>)</span><span class='hljl-oB'>/</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>-</span><span class='hljl-n'>alfa</span><span class='hljl-oB'>+</span><span class='hljl-n'>alfa</span><span class='hljl-oB'>*</span><span class='hljl-n'>epsilon</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>lamb</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>-</span><span class='hljl-n'>theta</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>-</span><span class='hljl-n'>bet</span><span class='hljl-oB'>*</span><span class='hljl-n'>theta</span><span class='hljl-p'>)</span><span class='hljl-oB'>/</span><span class='hljl-n'>theta</span><span class='hljl-oB'>*</span><span class='hljl-n'>THETA</span><span class='hljl-t'>
</span><span class='hljl-n'>kappa</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>lamb</span><span class='hljl-oB'>*</span><span class='hljl-p'>(</span><span class='hljl-n'>sig</span><span class='hljl-oB'>+</span><span class='hljl-p'>(</span><span class='hljl-n'>phi</span><span class='hljl-oB'>+</span><span class='hljl-n'>alfa</span><span class='hljl-p'>)</span><span class='hljl-oB'>/</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>-</span><span class='hljl-n'>alfa</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
0.12750000000000006
</pre>


<p>Gali &#40;2008&#41; gives an analytical solution to the model above. We implement this and it will be useful for checking whatever we implemented Gensys right, and it gives us some idea of the size of the numerical errors in the algorithm, if any.</p>
<h1>The software</h1>
<p>We implement all of this in Julia. Julia balances speed with a relatively easy to understand language: it might be slower than Fortran or C&#43;&#43;, however it is much easier to learn. Julia is shipped with many facilities for linear algebra that will be useful. Last, but not least, there is a package system for Julia just like R and a rich ecosystem. We will use many of the packages available to skip some boring but important steps, and just focus on the algorithms that are central to solving and estimating a DSGE. One exception is the Kalman Filter, which we use the implementation by Thomas Sargent.</p>
<h2>Gensys</h2>
<p>Gensys is a solver of linear rational expectation systems. It requires that we write the model using expectational errors, i.e. <span class="math">$\eta_y = E_{t}(y_{t+1}) - y_{t+1}$</span>. The cannonical form of a DSGE model for Gensys is:</p>
<p class="math">\[
\Gamma_0 y_t = \Gamma_1 y_{t-1} + \Psi \varepsilon_t + \Pi \eta_t
\]</p>
<p>In which <span class="math">$\varepsilon_t$</span> are fundamental shocks and <span class="math">$\eta_t$</span> are expectational shocks. We can rewrite the model from Gali&#39;s book using <span class="math">$\eta_t^{\pi} = \pi_t - E_{t-1} \pi_t$</span> and <span class="math">$\eta_t^{\tilde{y}} = \tilde{y}_t - E_{t-1} \tilde{y}_t$</span>, and we get:</p>
<p class="math">\[
\pi_{t-1} - \kappa \tilde{y}_{t-1} + \beta \eta_t^{\pi} = \beta \pi_t\\
\sigma\tilde{y}_{t-1} + i_{t-1} + \eta_t^{\pi} + \sigma \eta_t^{\tilde{y}} = \pi_t + \sigma \tilde{y}_t\\
i_{t-1} - \phi_{\pi}\pi_{t-1} - \phi_{\tilde{y}} \tilde{y}_{t-1} - v_{t-1} = 0\\
v_t = \rho v_{t-1} + \varepsilon^v_t
\]</p>
<p>And it is straight foward to write the matrices <span class="math">$\Gamma_0$</span>, <span class="math">$\Gamma_1$</span>, <span class="math">$\Psi$</span>, <span class="math">$\Pi$</span>, as follows:</p>


<pre class='hljl'>
<span class='hljl-n'>GAMMA_0</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>[</span><span class='hljl-n'>bet</span><span class='hljl-t'>    </span><span class='hljl-ni'>0</span><span class='hljl-t'>     </span><span class='hljl-ni'>0</span><span class='hljl-t'>  </span><span class='hljl-ni'>0</span><span class='hljl-p'>;</span><span class='hljl-t'>
           </span><span class='hljl-ni'>1</span><span class='hljl-t'>      </span><span class='hljl-n'>sig</span><span class='hljl-t'>   </span><span class='hljl-ni'>0</span><span class='hljl-t'>  </span><span class='hljl-ni'>0</span><span class='hljl-p'>;</span><span class='hljl-t'>
           </span><span class='hljl-ni'>0</span><span class='hljl-t'>      </span><span class='hljl-ni'>0</span><span class='hljl-t'>     </span><span class='hljl-ni'>0</span><span class='hljl-t'>  </span><span class='hljl-ni'>0</span><span class='hljl-p'>;</span><span class='hljl-t'>
           </span><span class='hljl-ni'>0</span><span class='hljl-t'>      </span><span class='hljl-ni'>0</span><span class='hljl-t'>     </span><span class='hljl-ni'>0</span><span class='hljl-t'>  </span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'>

</span><span class='hljl-n'>GAMMA_1</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-t'>      </span><span class='hljl-oB'>-</span><span class='hljl-n'>kappa</span><span class='hljl-t'>  </span><span class='hljl-ni'>0</span><span class='hljl-t'>  </span><span class='hljl-ni'>0</span><span class='hljl-p'>;</span><span class='hljl-t'>
           </span><span class='hljl-ni'>0</span><span class='hljl-t'>       </span><span class='hljl-n'>sig</span><span class='hljl-t'>    </span><span class='hljl-ni'>1</span><span class='hljl-t'>  </span><span class='hljl-ni'>0</span><span class='hljl-p'>;</span><span class='hljl-t'>
           </span><span class='hljl-oB'>-</span><span class='hljl-n'>phi_pi</span><span class='hljl-t'>  </span><span class='hljl-oB'>-</span><span class='hljl-n'>phi_y</span><span class='hljl-t'>  </span><span class='hljl-ni'>1</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-ni'>1</span><span class='hljl-p'>;</span><span class='hljl-t'>
           </span><span class='hljl-ni'>0</span><span class='hljl-t'>       </span><span class='hljl-ni'>0</span><span class='hljl-t'>      </span><span class='hljl-ni'>0</span><span class='hljl-t'>  </span><span class='hljl-n'>rho_v</span><span class='hljl-p'>]</span><span class='hljl-t'>

</span><span class='hljl-n'>PSI</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>[</span><span class='hljl-ni'>0</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-ni'>0</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-ni'>0</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'>

</span><span class='hljl-n'>PI</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>[</span><span class='hljl-n'>bet</span><span class='hljl-t'>  </span><span class='hljl-ni'>0</span><span class='hljl-p'>;</span><span class='hljl-t'>
      </span><span class='hljl-ni'>1</span><span class='hljl-t'>    </span><span class='hljl-n'>sig</span><span class='hljl-p'>;</span><span class='hljl-t'>
      </span><span class='hljl-ni'>0</span><span class='hljl-t'>    </span><span class='hljl-ni'>0</span><span class='hljl-p'>;</span><span class='hljl-t'>
      </span><span class='hljl-ni'>0</span><span class='hljl-t'>    </span><span class='hljl-ni'>0</span><span class='hljl-p'>]</span>
</pre>


<pre class="output">
4Ã—2 Array&#123;Float64,2&#125;:
 0.99  0.0
 1.0   1.0
 0.0   0.0
 0.0   0.0
</pre>


<p>Here are LaTeX versions of the matrices generated automatically with the package <em>latexify</em>, from the inputed matrices for Julia:</p>


<pre class='hljl'>
<span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>Latexify</span><span class='hljl-t'>

</span><span class='hljl-nf'>print</span><span class='hljl-p'>(</span><span class='hljl-nf'>string</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;</span><span class='hljl-se'>\$\$\\</span><span class='hljl-s'>Gamma_0 = </span><span class='hljl-se'>\$\$</span><span class='hljl-s'>&quot;</span><span class='hljl-p'>,</span><span class='hljl-nf'>latexify</span><span class='hljl-p'>(</span><span class='hljl-n'>GAMMA_0</span><span class='hljl-p'>)))</span>
</pre>


$$\Gamma_0 = $$\begin{equation}
\left[
\begin{array}{cccc}
0.99 & 0.0 & 0.0 & 0.0 \\
1.0 & 1.0 & 0.0 & 0.0 \\
0.0 & 0.0 & 0.0 & 0.0 \\
0.0 & 0.0 & 0.0 & 1.0 \\
\end{array}
\right]
\end{equation}



<pre class='hljl'>
<span class='hljl-nf'>print</span><span class='hljl-p'>(</span><span class='hljl-nf'>string</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;</span><span class='hljl-se'>\$\$\\</span><span class='hljl-s'>Gamma_1 = </span><span class='hljl-se'>\$\$</span><span class='hljl-s'>&quot;</span><span class='hljl-p'>,</span><span class='hljl-nf'>latexify</span><span class='hljl-p'>(</span><span class='hljl-n'>GAMMA_1</span><span class='hljl-p'>)))</span>
</pre>


$$\Gamma_1 = $$\begin{equation}
\left[
\begin{array}{cccc}
1.0 & -0.12750000000000006 & 0.0 & 0.0 \\
0.0 & 1.0 & 1.0 & 0.0 \\
-1.5 & -0.125 & 1.0 & -1.0 \\
0.0 & 0.0 & 0.0 & 0.5 \\
\end{array}
\right]
\end{equation}



<pre class='hljl'>
<span class='hljl-nf'>print</span><span class='hljl-p'>(</span><span class='hljl-nf'>string</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;</span><span class='hljl-se'>\$\$\\</span><span class='hljl-s'>Psi = </span><span class='hljl-se'>\$\$</span><span class='hljl-s'>&quot;</span><span class='hljl-p'>,</span><span class='hljl-nf'>latexify</span><span class='hljl-p'>(</span><span class='hljl-n'>PSI</span><span class='hljl-p'>)))</span>
</pre>


$$\Psi = $$\begin{equation}
\left[
\begin{array}{c}
0 \\
0 \\
0 \\
1 \\
\end{array}
\right]
\end{equation}



<pre class='hljl'>
<span class='hljl-nf'>print</span><span class='hljl-p'>(</span><span class='hljl-nf'>string</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;</span><span class='hljl-se'>\$\$\\</span><span class='hljl-s'>Pi = </span><span class='hljl-se'>\$\$</span><span class='hljl-s'>&quot;</span><span class='hljl-p'>,</span><span class='hljl-nf'>latexify</span><span class='hljl-p'>(</span><span class='hljl-n'>PI</span><span class='hljl-p'>)))</span>
</pre>


$$\Pi = $$\begin{equation}
\left[
\begin{array}{cc}
0.99 & 0.0 \\
1.0 & 1.0 \\
0.0 & 0.0 \\
0.0 & 0.0 \\
\end{array}
\right]
\end{equation}


<p>Gensys allows <span class="math">$\Gamma_0$</span> to be singular. To obtain a system without <span class="math">$\Gamma_0$</span>, it relies on the generalized Schur decomposition for both <span class="math">$\Gamma_0$</span> and <span class="math">$\Gamma_1$</span>, simultaneously. The <strong>LinearAlgebra</strong> package that comes with Julia. To test whatever the system has a solution and if it is unique, Gensys <em>does not</em> relies on the usual Blanchard Khan condition i.e. whatever the number of explosive eigenvalues is equal to the number of jumping variables - in fact, Gensys does not require that one defines which variables are jumping and which are pre determined.</p>
<h2>Likelihood</h2>
<p>The likelihood function is the next step of our code. The inputs are a set of parameter values and data regarding the economy. For a set of parameters, we use gensys to find the VAR representation of the model. We could estimate it as an usual VAR, however:</p>
<ol>
<li><p>In general, the model assumes that the shock is an autocorrelated process. This means that the true process is a VARMA model and the coefficient of the autocorrelation and the fundamental shocks must be estimated as well.</p>
</li>
<li><p>For estimation, one can only include as many data series as there are shocks in the model. In this extremely simple example, this forces us to work with a single series. Since the dynamic of the model depends on the evolution of all the series, we have to estimate this series from the data.</p>
</li>
</ol>
<p>The fact that the model is linear with gaussian shocks allows us to work with the Kalman Filter, that adresses both points above. The Kalman Filter work from the state space representation of the model:</p>
<p class="math">\[
y_t = Gx_t + v_t\\
x_t = Ax_{t-1} + w_t
\]</p>
<p>In which <span class="math">$v_t ~ N(0,R)$</span> and <span class="math">$w_t ~ N(0,Q)$</span>. The first equation is the <em>observation equation</em> and the second one is the <em>state equation</em>. The idea is that we observe <span class="math">$y_t$</span> and that the behaviour of the system is governed by <span class="math">$x_t$</span> - an idea closely related to the factor model in economics - and that we know <span class="math">$G$</span>,<span class="math">$A$</span>,<span class="math">$R$</span> and <span class="math">$Q$</span>. This allows us to recover <span class="math">$x_t$</span> in a recursive manner.</p>
<p>What we do is to set <span class="math">$A$</span> equals to <span class="math">$\Theta_1$</span> and <span class="math">$Q$</span> equals to <span class="math">$\sigma^2 \Theta_2 \Theta_2'$</span>, in which <span class="math">$\Theta_1$</span> and <span class="math">$\Theta_2$</span> are the matrices from gensys and <span class="math">$\sigma^2$</span> is the variance from the monetary policy shock. So our state space is the whole system. <span class="math">$G$</span> is just a selection matrix, i.e. which of the series from the state space we observe - this is filled with a value of 1 and everything else is just 0.</p>
<p>The value of <span class="math">$R$</span> is <em>not</em> set to zero. The usual interpration of the shock <span class="math">$v_t$</span> is that it is a measurement error, i.e. we do not observe the exatcly true GDP that our model prescribes because it has measurement errors. However, in our setup there are no such things: we simulate the values from the system and observe them exatcly. So, we should set it to zero.</p>
<p>However, it is well known &#40;see, for example, <strong>??????</strong>&#41; that if there are no measurement errors the Kalman filter suffers from serious numerical instability problems. One that we found is that it often happens that the forecast variance matrix will have a determinant close to zero if there are no measurement errors. To avoid this, we add a measurement error with variance <span class="math">$10^{-8}$</span>. This creates a really small error and avoids the numerical instability problems completely. We could push it even closer to zero, however due to the finite precision of floating point arithmetics we do not do it. There are ways of avoiding this - <strong>??????????</strong> suggests a root Kalman filter that is more stable from the numerical point of view.</p>
<p>Each step of the Kalman filter uses the current distribution about the state to forecast the state in the future. The error from the difference of the forecasted state to the real value of the state gives the mean of the normal. The forecast error variance is <span class="math">$P = G*\Sigma_s*G' + R$</span>. This gives us all we need to evaluate the likelihood at each observation. With the likelihood at each point, one can then <em>multiply</em> all of the values.</p>
<p>This is problematic because, in many cases, the value of the density at the point is smaller than 1. Since the computer has a finite decimal precision &#40;known as the <em>eps</em>&#41;, multiplying several numbers bellow one can quickly become messy. Just as an example, taking <span class="math">$0.0001^{100}$</span> is bellow the eps of most CPUs. To avoid this, is better to work with the log of the likelihood and sum them over. That is the procedue we follow.</p>
<p>We also throw away the first ten likelihoods from the state. One needs to initialize the state distribution at some value - since we are working with the Normal distribution, this means the mean and the variance of the distribution. It takes a while to make this values converge, and in some cases this throws non sense values that destroy the loglikelihood &#40;like <span class="math">$10^{18}$</span>&#41;.</p>
<h2>Markov Chain Monte Carlo</h2>
<p>Bayes&#39;s Theorem states that:</p>
<p class="math">\[
P(\Theta|y) = \frac{\ell(y|\Theta)p(\theta)}{P(y)}
\]</p>
<p>In which <span class="math">$\Theta$</span> is the vector of parameters, <span class="math">$y$</span> is the data, <span class="math">$\ell$</span> is the likelihood function, <span class="math">$p(\Theta)$</span> is the prior over the parameters and <span class="math">$P(y)$</span> is the marginal over the data. <span class="math">$P(y)$</span> actually depends of integrals over all the parameters, and so we suffer the curse of dimensionality. Markov Chain Monte Carlo &#40;MCMC&#41; solve this by avoiding estimating this object and instead relying on the reason <span class="math">$\frac{\ell(y|\Theta)p(\theta)}{\ell(y|\Theta')p(\theta')}$</span> to build the posterior. The algorithm is rather simple. Besides the prior and the likelihood, we will also need a kernel distribution, that we will call <span class="math">$\mathcal{K}(\Theta|\Theta')$</span>. Here is the algorithm for a special and rather useful algorithm for MCMC, the Metropolis-Hasting:</p>
<p>For j in 1:ndraws:</p>
<ol>
<li><p>Draw a vector of parameters <span class="math">$\Theta^{*}$</span>, from the distribution <span class="math">$\mathcal{K}(\Theta|\Theta_j)$</span>.</p>
</li>
<li><p>Compute <span class="math">$r = \min\left\{\frac{\ell(y|\Theta^*)p(\Theta^*)\mathcal{K}(\Theta_{j-1}|\Theta^*)}{\ell(y|\Theta_{j-1})p(\Theta_{j-1})\mathcal{K}(\Theta^*|\Theta_{j-1})},1\right\}$</span></p>
</li>
<li><p>Draw a random number from a <span class="math">$\alpha = U(0,1)$</span></p>
</li>
<li><p>If <span class="math">$r < \alpha$</span>, set <span class="math">$\Theta_j = \Theta_{j-1}$</span>; otherwise, set <span class="math">$\Theta_j = \Theta^{*}$</span></p>
</li>
</ol>
<p>end</p>
<p>In the end we have <span class="math">$\{\Theta_j\}_{j=1}^{ndraws}$</span>, a set of ndraws of the parameter vector.This algorithm maps the posterior, and is quite smart: every time you draw a parameter vector that makes the likelihood times the prior increase a lot compared to the point you are now, you accept. However, even it does not increase, this does not means that the vector will be excluded. It has some probability of being included. This allows the algorithm to map the distribution and to not get stuck in some points of high probability, like the mode.</p>
<p>The choice of the kernel is important. Usually, the kernel is a Normal distribution that has mean in the last parameter vector accepted &#40;therefore the random walk part of the name&#41;. The choice of the variance is of central importance: the optimal choice for the variance of the kernel is <span class="math">$c\Sigma_\Theta$</span>, in which <span class="math">$\Sigma_\Theta$</span> is the variance matrix for the vector <span class="math">$\Theta$</span> evaluated on the mode. <span class="math">$c$</span> is a scale constant that plays a fundamental role to ensure that we will map the whole distribution. The idea is simple: if the variance is too low, you will visit only a small part of the distribution in a given number of iterations; set it too high and the algorithm will start to wander in regions far away from the mode, that contribute little to the overall density. Therefore setting <span class="math">$c$</span> in the right way ensures that our iterations won&#39;t be wasted.        </p>
<p>Notice that our previous discussion about multiplying small numbers also applys when taking the ratio of two &#40;potentialy&#41; small numbers. So, since we could get the likelihood from the log likelihood &#40;just apply the exponential&#41;, we do not do it. Instead, step 3 becomes:</p>
<p class="math">\[
r = \min\{0,\log(\ell(y|\Theta^{*})) + \log(p(\Theta^{*})) + \log(\mathcal{K}(\Theta_{j-1}|\Theta^*)) - \log(\ell(y|\Theta_{j-1})) + \log(p(\Theta_{j-1})) + \log(\mathcal{K}(\Theta^{*}|\Theta_{j-1}))\}
\]</p>
<p>And we use <span class="math">$\log(\alpha)$</span> instead of <span class="math">$\alpha$</span> in step 4.</p>



          <HR/>
          <div class="footer"><p>
          Published from <a href="dsge_and_julia.jmd">dsge_and_julia.jmd</a> using
          <a href="http://github.com/mpastell/Weave.jl">Weave.jl</a>
           on 2019-12-18.
          <p></div>


        </div>
      </div>
    </div>
  </BODY>
</HTML>
