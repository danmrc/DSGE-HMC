---
title: HMC & DSGE - Take 2
author: Gilberto Boaretto & Daniel Coutinho
date: September, 2020
weave_options:                
  md2tex:
    out_path: output/hmc_take2.tex
  md2html:
      out_path: output/hmc_take2.html
  out_path: output/hmc_take2.md
---

Almost all derivates are now implemented analytically. There are two files for it: `diffs_estrut.jl`, for the derivates of the DSGE model, following Iskrev (2010) and `diff_kalman.jl`, that take the derivate with respect to the Kalman Filter equations. I will review them bellow.

There is the need to make the parameters all belong to the whole real line and so we reparametrize them. This was done using a Julia package, but to be able to handle the analytical derivates they are now done by hand (without the package). I review those in the third section.

We need to derivate the posterior and priors. This is review in the fourth section.

# Analytical derivates

## DSGE model

We follow Iskrev (2010). We will write a macro model in structural form with $z_t$ being a $m$ dimensional vector of endogenous variables:

$$\Gamma_0(\theta)z_t = \Gamma_1(\theta) E_t(z_{t+1}) + \Gamma_2(\theta) z_{t-1} + \Gamma_3 (\theta) u_t$$

And the reduced form is:

$$
z_t = A(\tau)z_{t-1} + B(\tau) u_t
$$

Let $x_t$ be the observed variables, which is $l$ dimensional, so the measurement equation is $x_t = C(\theta) z_t$. Usually, $C(\theta)$ is just a matrix of 1 and 0.

We also establish that $\Omega = B^{\prime}B$, $\Sigma_z = A\Sigma_zA^{\prime} + \Omega$ and $Ex_{t+i}x_t = \Sigma_x(i)$, which is:

$$
\Sigma_x(i) = \begin{cases}
C\Sigma_zC^{\prime} \quad \text{if } i = 0\\
CA^{i}\Sigma_zC^{\prime} \quad \text{otherwise}
\end{cases}
$$

And define the vector $\sigma_T = \left[vech(\Sigma_x(0))^{\prime}\ vech(\Sigma_x(1))^{\prime}\ ...\ vech(\Sigma_x(T-1))^{\prime}\right]$ and $\tau = \left[vec(A)^{\prime} \ vec(C)^{\prime} \ vech(\Omega)^{\prime}\right]$. To obtain the effects of the structural form in the reduced form we will use the chain rule:

$$
J(\tau) = \underbrace{\dfrac{\partial \sigma_T}{\partial \tau}}_{J_1} \underbrace{\dfrac{\partial \tau}{\partial \theta}}_{J_2}
$$

To start take the reduced form:

$$
z_t = A(\tau)z_{t-1} + B(\tau) u_t
$$

Put everything forward one period:

$$
z_{t+1} = A(\tau)z_{t} + B(\tau) u_{t+1}
$$

So taking the expectation, we have:

$$
E_t(z_{t+1}) = A(\tau)z_{t}
$$

So plug this on the structural form:

$$
\Gamma_0(\theta)z_t = \Gamma_1(\theta) A(\tau)z_t + \Gamma_2(\theta) z_{t-1} + \Gamma_3 (\theta) u_t \\
(\Gamma_0(\theta)- \Gamma_1(\theta) A(\tau))z_t = \Gamma_2(\theta) z_{t-1} + \Gamma_3 (\theta) u_t
$$

Lets start with $J_1 = \dfrac{\partial \sigma_T}{\partial \tau} = \dfrac{\partial}{\partial \tau} \left[vech(\Sigma_x(0))^{\prime}\ vech(\Sigma_x(1))^{\prime}\ ...\ vech(\Sigma_x(T-1))^{\prime}\right]$ and recall that:

$$
\Sigma_x(i) = \begin{cases}
C\Sigma_zC^{\prime} \quad \text{if } i = 0\\
CA^{i}\Sigma_zC^{\prime} \quad \text{otherwise}
\end{cases}
$$

So we can take the derivate of each block.

### $J_2$

Previously we found that:

$$
(\Gamma_0(\theta)- \Gamma_1(\theta) A(\theta))z_t = \Gamma_2(\theta) z_{t-1} + \Gamma_3 (\theta) u_t
$$

And we had the reduced form:

$$
z_t = A(\tau)z_{t-1} + B(\tau)u_t
$$

Plug it on the $z_t$ above and we will have:

$$[\Gamma_0(\theta)- \Gamma_1(\theta) A(\tau)][A(\theta)z_{t-1} + B(\theta)u_t] = \Gamma_2(\theta) z_{t-1} + \Gamma_3 (\theta) u_t$$

Rearranging:

$$
[(\Gamma_0(\theta) - \Gamma_1(\theta)A(\tau))A(\tau) - \Gamma_2(\theta)]z_{t-1} + [(\Gamma_0(\theta)-\Gamma_1(\theta)A(\tau))B(\tau) - \Gamma_3(\theta)]u_t
$$

So we must have:

$$
\begin{cases}
[\Gamma_0(\theta) - \Gamma_1(\theta)A(\tau)]A(\tau) - \Gamma_2(\theta) = 0 := F_1\\
[\Gamma_0(\theta)-\Gamma_1(\theta)A(\tau)]\Omega(\tau)[\Gamma_0(\theta)-\Gamma_1(\theta)A(\tau)]^{\prime} - \Gamma_3(\theta)\Gamma_3(\theta)^{\prime} = 0 := F_2
\end{cases}
$$

So we can use the implicit function theorem on $f = [F_1^{\prime} \; F_2^{\prime}]^{\prime}$ to get:

$$\frac{\partial \theta}{\partial \tau} = \left(\frac{\partial f}{\partial \theta}\right)^{-1}\dfrac{\partial f}{\partial \tau}$$

So we will need $\frac{\partial F_1}{\partial \theta}$, $\frac{\partial F_2}{\partial \theta}$, $\frac{\partial F_1}{\partial \tau}$,$\frac{\partial F_2}{\partial \tau}$. Notice that $F_2$ is symmetric and we can use $vech$

#### $\dfrac{\partial F_1}{\partial \tau}$

$$\dfrac{\partial F_1}{\partial \tau} = -\Gamma_1 (dA) A - (\Gamma_0 - \Gamma_1 A)dA \therefore \\
dvec(F_1) = - (A^{\prime} \otimes \Gamma_1) dvec(A) - (I_m \otimes (\Gamma_0 - \Gamma_1 A)) dvec(A) = \\
dvec(F_1) = (-A^{\prime} \otimes \Gamma_1 - I_m \otimes \Gamma_0 + I_m \otimes \Gamma_1A)dvec(A)$$

We will use `l` as the number of observable variables and `n` is the number of shocks. Here is the code:

```{julia}

function dF1_tau(Gamma_0, Gamma_1, A,l)
    # l is the number of observables
    # n is the number of shocks
    m = size(Gamma_0,1)
    nn = Int(m*(m+1)/2)
    dvecA = [I(m^2) zeros(m^2,m*l+nn)]
    (kron(-A', Gamma_1) - kron(I(m), Gamma_0) + kron(I(m),Gamma_1*A))* dvecA
end

```

#### $\dfrac{\partial F_1}{\partial \theta}$

$$\dfrac{\partial F_1}{\partial \theta} = [d\Gamma_0 - d\Gamma_1A]A - d\Gamma_2 = d\Gamma_0 A - d\Gamma_1A^2 - d\Gamma_2 \therefore\\
dvec(F_1) = (A^\prime \otimes I_m)dvec(\Gamma_0) - (A^{\prime2} \otimes I_m) dvec(\Gamma_1) - dvec(\Gamma_2)$$

The function that does that:

```{julia}

function dF1_theta(dGamma_0,dGamma_1,dGamma_2,A)
    m = size(A,1)
    #dv_size = size(dGamma_0,1)*size(dGamma_0,2)
    #dGamma_0 = vecc(dGamma_0)
    #dGamma_1 = vecc(dGamma_1,dv_size,1)
    #dGamma_2 = vecc(dGamma_2)
    kron(A',I(m))* dGamma_0 - kron(A'*A',I(m))* dGamma_1 - dGamma_2
end
```


#### $\dfrac{\partial F_2}{\partial \tau}$

Recall that $F_2$ is:

$$
F_2 := [\Gamma_0(\theta)-\Gamma_1(\theta)A(\tau)]\Omega(\tau)[\Gamma_0(\theta)-\Gamma_1(\theta)A(\tau)]^{\prime} - \Gamma_3(\theta)\Gamma_3(\theta)^{\prime} = 0
$$

So:

$$
\frac{\partial F_2}{\partial \tau} = -\Gamma_1(dA) \Omega [\Gamma_0 - \Gamma_1 A]^{\prime} - [\Gamma_0 - \Gamma_1 A] \Omega (dA^{\prime}) \Gamma_1^{\prime} + [\Gamma_0 - \Gamma_1 A]d\Omega [\Gamma_0 - \Gamma_1 A]^{\prime}
$$

Passing vec on everything:

$$
dvec(F_2) = -[(\Gamma_0 - \Gamma_1A)\Omega \otimes \Gamma_1] dvec(A) - (\Gamma_1 \otimes [\Gamma_0 - \Gamma_1 A]\Omega)dvec(A^\prime) + ([\Gamma_0 - \Gamma_1 A] \otimes [\Gamma_0 - \Gamma_1 A]) dvec(\Omega)
$$

Now $vech$ stuff:

$$
D_mdvech(F_2) = -[(\Gamma_0 - \Gamma_1A)\Omega \otimes \Gamma_1] dvec(A) - [\Gamma_1 \otimes (\Gamma_0 - \Gamma_1 A)\Omega]dvec(A^\prime) + [(\Gamma_0 - \Gamma_1 A) \otimes (\Gamma_0 - \Gamma_1 A)] D_mdvec(\Omega) = \\
-[(\Gamma_0 - \Gamma_1A)\Omega \otimes \Gamma_1 - (\Gamma_1 \otimes (\Gamma_0 - \Gamma_1 A)\Omega)K_{mm}]dvec(A) + [(\Gamma_0 - \Gamma_1 A) \otimes (\Gamma_0 - \Gamma_1 A)] D_mdvec(\Omega)
$$

Notice that the code is implemented using a vech:

```{julia}

function dF2_tau(Gamma_0, Gamma_1, Gamma_2, A, Omega,l)
    m = size(Gamma_0,1)
    b_aux = (Gamma_0 - Gamma_1*A)*Omega
    Dm = duplication_matrix(m)
    Kmm = commutation_matrix(m,m)
    nn = Int(m*(m+1)/2)

    dvecA = [I(m^2) zeros(m^2,m*l+nn)]
    dvecOmega = [zeros(nn,m^2+m*l) I(nn)]
    res = -(kron(b_aux,Gamma_1) + kron(Gamma_1, b_aux)*Kmm)*dvecA + kron(Gamma_0 - Gamma_1*A,Gamma_0 - Gamma_1*A)*Dm*dvecOmega
    return(pinv(Dm)*res)
end
```

#### $\dfrac{\partial F_2}{\partial \theta}$

Using the definition of $F_2$ above, we have:

$$
\frac{\partial F_2}{\partial \theta} = d\Gamma_0 \Omega (\Gamma_0 - \Gamma_1A)^{\prime} - d\Gamma_1A\Omega(\Gamma_0 - \Gamma_1 A)^{\prime} + (\Gamma_0 - \Gamma_1 A) \Omega d\Gamma_0^{\prime} - (\Gamma_0 - \Gamma_1 A)\Omega (d\Gamma_1 A)^{\prime} - d\Gamma_3 \Gamma_3 - \Gamma_3 (d\Gamma_3)^{\prime}
$$

Now $vec$ everything:

$$
\frac{\partial vec(F_2)}{\partial \theta} = ((\Gamma_0 - \Gamma_1A)\Omega \otimes I_m)dvec(\Gamma_0)  - ((\Gamma_0 - \Gamma_1 A)\Omega{}A^{\prime} \otimes I_m)dvec(\Gamma_1) + (I_m \otimes (\Gamma_0 - \Gamma_1 A) \Omega) dvec(\Gamma_0^{\prime}) - (I_m \otimes (\Gamma_0 - \Gamma_1 A)\Omega) dvec(\Gamma_1^{\prime}) - (\Gamma_3 \otimes I_m) dvec(\Gamma_3)  - (I_m \otimes \Gamma_3) dvec(\Gamma_3^{\prime})
$$

Use the commutation matrix to group the terms that depend on the derivate of the transpose:

$$
\frac{\partial vec(F_2)}{\partial \theta} = ((\Gamma_0 - \Gamma_1A)\Omega \otimes I_m)dvec(\Gamma_0)  - ((\Gamma_0 - \Gamma_1 A)\Omega{}A^{\prime} \otimes I_m)dvec(\Gamma_1) + (I_m \otimes (\Gamma_0 - \Gamma_1 A) \Omega)K_{mn} dvec(\Gamma_0) - (I_m \otimes (\Gamma_0 - \Gamma_1 A)\Omega) K_{mm} dvec(\Gamma_1) - (\Gamma_3 \otimes I_m) dvec(\Gamma_3)  - (I_m \otimes \Gamma_3) K_{mm}dvec(\Gamma_3) \\
= [(\Gamma_0 - \Gamma_1A)\Omega \otimes I_m + (I_m \otimes (\Gamma_0 - \Gamma_1 A) \Omega{}) K_{mn}]dvec(\Gamma_0)  - [(\Gamma_0 - \Gamma_1 A)\Omega{}A^{\prime} \otimes I_m + (I_m \otimes (\Gamma_0 - \Gamma_1 A)\Omega) K_{mm}]dvec(\Gamma_1)   - [\Gamma_3 \otimes I_m + (I_m \otimes \Gamma_3) K_{mm}]dvec(\Gamma_3)
$$

Use $vech$ where you can and the duplication matrix:

$$
D_m\frac{\partial vech(F_2)}{\partial \theta} = [(\Gamma_0 - \Gamma_1A)\Omega \otimes I_m + (I_m \otimes (\Gamma_0 - \Gamma_1 A) \Omega{}) K_{mn}]dvec(\Gamma_0)  - [(\Gamma_0 - \Gamma_1 A)\Omega{}A^{\prime} \otimes I_m + (I_m \otimes (\Gamma_0 - \Gamma_1 A)\Omega) K_{mm}]dvec(\Gamma_1)   - [\Gamma_3 \otimes I_m + (I_m \otimes \Gamma_3) K_{mn}]dvec(\Gamma_3)
$$

And the function in Julia:

```{julia}

function dF2_theta(Gamma_0, Gamma_1, Gamma_3, dGamma_0, dGamma_1, dGamma_3, Omega, A)
    m = size(Gamma_0,1)
    b_aux = Gamma_0 - Gamma_1*A
    Dm = duplication_matrix(m)
    Kmm = commutation_matrix(m,m)

    dv_size = size(dGamma_0,1)*size(dGamma_0,2)
    #dGamma_0 = vecc(dGamma_0)
    #dGamma_1 = vecc(dGamma_1)
    #dGamma_2 = vecc(dGamma_2)

    b1 = kron(b_aux*Omega, I(m)) + kron(I(m), b_aux*Omega)*Kmm
    b2 = kron(b_aux*Omega*A',I(m)) + kron(I(m),b_aux*Omega)*Kmm
    b3 = kron(Gamma_3,I(m)) + kron(I(m),Gamma_3)*Kmm
    res = b1*dGamma_0 - b2*dGamma_1 - b3*dGamma_3
    res = pinv(Dm)*res
end
```

The following function join all the pieces:

```{julia}

function dtheta(Gamma_0,Gamma_1,Gamma_2,Gamma_3, dGamma_0, dGamma_1, dGamma_2, dGamma_3, A, Omega,l)
    Df_theta = [dF1_theta(dGamma_0,dGamma_1,dGamma_2,A);dF2_theta(Gamma_0, Gamma_1, Gamma_3, dGamma_0, dGamma_1, dGamma_3, Omega, A)]
    Df_tau = [dF1_tau(Gamma_0, Gamma_1, A,l); dF2_tau(Gamma_0, Gamma_1, Gamma_2, A, Omega,l)]
    return pinv(Df_theta)*Df_tau
end
```



#### Derivates of reduced form matrices

We have that:

$$
\frac{dvec(A)}{d \tau} = [I_{m^2},\mathbf{0}_{lm \times lm}, \mathbf{0}_{n^2(n+1)^2/4,n^2(n+1)^2/4}]\\
\frac{dvec(C)}{d\tau} = [\mathbf{0}_{m^2 \times m^2}, I_{lm},  \mathbf{0}_{n^2(n+1)^2/4,n^2(n+1)^2/4}] \\
\frac{dvec(\Omega)}{d\tau} = [\mathbf{0}_{m^2 \times m^2},\mathbf{0}_{lm \times lm}, I_{n^2(n+1)^2/4}] \\
$$


## Kalman Filter

I followed a somewhat obscure book called _Estimation of Dynamic Econometric Models with Errors in Variables_ (by Jaime Terceiro Lomba) that has the derivates of the State Space Model. Fernandez-Villaverde paper suggests a paper by Mark Watson on that. We will use the state space model:

$$
y_t = Gx_t + v_t, \quad v_t \sim N(0,R)\\
x_{t+1} = Ax_t + w_{t+1} \quad v_t \sim N(0,Q)
$$

Let the state have dimension $n$ and the observable variable $m$. We will define and use the following:

$$
\tilde{y}_t = y_t - Gx_t \quad \quad (2.1)\\
x_{t+1} = Ax_t + K_t \tilde{y}_t \quad \quad (2.2)\\
K_t = (AP_{t|t-1}G^{\prime})B_t^{-1} \quad \quad (2.3)\\
P_{t+1|t} = AP_{t|t-1}A^{\prime} + Q - K_tB_tK_t^{\prime}\quad \quad (2.4)\\
B_t = GP_{t|t-1}G^{\prime} + R \quad \quad (2.5)\\
$$

In which $K_t$ is the Kalman gain.

Starting with 2.1:

$$\frac{\partial \tilde{y}_t}{\partial \theta} = -dGx_t - G\frac{\partial x_t}{\partial \theta} \quad (i)$$

Now pass vec on everything:

$$\frac{\partial \tilde{y}_t}{\partial \theta} = (x_t' \otimes I_m) dvec(G) - G \frac{\partial x_t}{\partial \theta}$$

The code that does that:

```{julia}

function grad_y(x_hat_l,grad_x_hat_l,G, dG)
    m = size(G,1)
    return transpose(kron(x_hat_l', I(m))*dG'-G*grad_x_hat_l')
end
```

Now 2.2:

$$\frac{\partial x_{t+1}}{\partial \theta} = dA x_t + A \frac{\partial x_t}{\partial \theta} + dK_t \tilde{y}_t + K_t d\tilde{y}_t \quad(ii)$$

Vec stuff:

$$\frac{\partial x_{t+1}}{\partial \theta} = (x_t^{\prime} \otimes I_n) dvec(A) + A \frac{\partial x_t}{\partial \theta} + (\tilde{y}_t \otimes I_n) dvec(K_t) + K_t \frac{\partial \tilde{y}_t}{\partial \theta}$$

And the code:

```{julia}

function grad_x_hat(x_hat_l, dA,A,grad_x_hat_l, y, grad_y, K,dK)
    n = size(A,1)
    return transpose(kron(x_hat_l',I(n))*dA' + A*grad_x_hat_l' + kron(y',I(n))*dK' + K*grad_y')
end
```

We will need the derivate of $K_t$, which is expression 2.3:

$$dK_t = (dAP_{t|t-1}G^{\prime} + AdP_{t|t-1}G^{\prime})B_t^{-1} + (AP_{t|t-1}G^{\prime})dB_t^{-1}$$

Now using the fact that $dB_t^{-1} = - B_t^{-1}dB_t B_t^{-1}$, we have:

$$dK_t = (dAP_{t|t-1}G^{\prime} + AdP_{t|t-1}G^{\prime})B_t^{-1} -(AP_{t|t-1}G^{\prime})B_t^{-1}dB_tB_t^{-1}$$

Passing vec on everything, we have:

$$dvec(K_t) = ((P_{t|t-1}G^{\prime}B_t^{-1}) \otimes I_n) dvec(A) + (B_t^{-1}G \otimes A) dvec(P_{t|t-1}) + (B_t^{-1} \otimes AP_{t|t-1}G^{\prime}B_t^{-1}) dvec(B_t)$$

And the function that does that:

```{julia}

function diff_K(A,P,G,S_l,dA,dG,dP,dS_l)
    m = size(G,1)
    n = size(A,1)
    P_inv = inv(P)
    Kmn = commutation_matrix(m,n)
    return transpose(kron(P_inv*G*S_l,I(n))*dA' + kron(P_inv*G,A)*dS_l'+kron(P_inv,A*S_l)*Kmn*dG' - kron(P_inv,A*S_l*G'*P_inv)*dP')
end
```

Next, we need $dP_{t+1|t}$:

$$dP_{t+1|t} = dAP_{t|t-1}A^{\prime} + AdP_{t|t-1}A^{\prime} + AP_{t|t-1}dA^{\prime} + dQ - dK_tB_tK_t^{\prime} - K_tdB_tK_t^{\prime} - K_tB_tdK_t^{\prime}$$

Using vec on everything:

$$
dvec(P_{t+1|t}) = (AP_{t|t-1} \otimes I) dvec(A) + (A \otimes A) dvec(P_{t|t-1}) + (I \otimes P_{t|t-1}A^{\prime}) dvec(A^{\prime}) + dvec(Q) - (K_tB_t \otimes I) dvec(K_t) + (K_t \otimes K_t) dvec(B_t) + (I \otimes K_tB_t) dvec(K_t^{\prime})
$$

We can use the commutation matrix to get:

$$
dvec(P_{t+1|t}) = [(AP_{t|t-1} \otimes I) + (I \otimes P_{t|t-1}A^{\prime})K_{mn}] dvec(A) + (A \otimes A) dvec(P_{t|t-1}) +  + dvec(Q) - [(K_tB_t \otimes I) + (I \otimes K_tB_t)K_{mn}] dvec(K_t) + (K_t \otimes K_t) dvec(B_t)
$$

And the code:

```{julia}

function diff_S(A,P,K,S_l,dA,dP,dK,dS_l,dQ)
    m = size(P,1)
    n = size(A,1)
    Knn = commutation_matrix(n,n)
    Kmn = commutation_matrix(m,n)
    b1 = (kron(A*S_l,I(n)) + kron(I(n),A*S_l)*Knn)*dA'
    b2 = (kron(K*P,I(n)) + kron(I(n),K*P)*Kmn)*dK'
    return transpose(b1 + kron(A,A)*dS_l' -b2 - kron(K,K)*dP' +duplication_matrix(n)*dQ')
end
```

Equation 2.5 is the final one we need to take the derivate:

$$
dB_t = dGP_{t|t-1}G^{\prime} +GdP_{t|t-1}G^{\prime} + GP_{t|t-1}dG^{\prime} + dR
$$

Now to vec stuff:

$$
dvec(B_t) = (GP_{t|t-1} \otimes I) dvec(G) + (G \otimes G) dvec(P_{t|t-1}) + (I \otimes GP_{t|t-1}) dvec(G^{\prime}) + dvec(R)
$$

Using the commutation matrix:

$$
dvec(B_t) = [(GP_{t|t-1} \otimes I) + (I \otimes GP_{t|t-1}) K_{mn}] dvec(G) + (G \otimes G) dvec(P_{t|t-1})  + dvec(R)
$$

Finally, the code:

```{julia}

function diff_P(G,S_l,dG,dS_l,dR)
    m = size(G,1)
    n = size(G,2)
    Kmn = commutation_matrix(m,n)
    b1 = (kron(G*S_l,I(m)) + kron(I(m),G*S_l)*Kmn)*dG'
    return transpose(b1 + kron(G,G)*dS_l' + dR')
end
```

# Log likelihood & Reparametrization

Let $f_{\theta}(\theta, y_t|Y_{t-1})$ be the density of $y_t$ given $Y_{t-1} = \{y_j\}_{j=1}^{t-1}$. We will drop the subscript in $f$ when there is no ambiguity. We can write the likelihood as:

$$L(\theta) = \prod_{t=1}^{T} f(\theta,y_t|Y_{t-1})$$

We will work with a reparametrization, which we will call $h:A -> \mathbb{R}^p$, in which $p$ is the number of parameters. We can take $h_k$ that maps each paramater to the real line, so the jacobian matrix of the transformation will be a diagonal matrix. Let $J$ be the jacobian matrix, $\tilde{\theta}$ be the new parameters and $|J|$ the absolute value of the determinant of jacobian matrix (the determinant is also called the Jacobian, which is somewhat confusing). Then, we will work with:

$$f_{\tilde{\theta}}(\tilde{\theta},y_t|Y_{t-1}) = f_{\theta}(h^{-1}(\tilde{\theta}),y_t|Y_{t-1})|J|$$    

Plugging it on the likelihood, we will have:

$$L(\tilde{\theta}) = \prod_{i=1}^{T} f_{\theta}(h^{-1}(\tilde{\theta}),y_t|Y_{t-1})|J|$$

We want to work with the log likelihood, $\ell(\tilde{\theta})$:

$$
\ell(\tilde{\theta}) := \sum_{t=1}^T \left(\log(f_{\theta}(h^{-1}(\tilde{\theta}),y_t|Y_{t-1})) + \log(|J|)\right) = \\
= \sum_{t=1}^T \log(f_{\theta}(h^{-1}(\tilde{\theta}),y_t|Y_{t-1}))) + T\log(|J|)
$$

Since we will work with $\log(f)$, lets call it $\mathcal{F}$:

$$
\ell(\tilde{\theta}) = \sum_{t=1}^T \mathcal{F}(h^{-1}(\tilde{\theta}),y_t|Y_{t-1}))) + T\log(|J|)
$$

When we take the derivate we will have the hard problem of dealing with the derivate of the absolute value function. There are two solutions: (a) ignore and let software solve and pray for the best (b) argue that we can use sub-gradients since $abs$ is convex. We do (a) knowing (b). So the partial derivate is:

$$
\frac{\partial \ell}{\partial \tilde{\theta}_j} = \sum_{t=1}^{T} \frac{\partial\mathcal{F}(h^{-1}(\tilde{\theta}))}{\partial \tilde{\theta}_j}\frac{\partial h^{-1}(\tilde{\theta})}{\partial \tilde{\theta}_j} + T \frac{\partial \log(|J|)}{\partial \tilde{\theta}_j}
$$

And we now that:

$$\frac{\partial \log(|J|)}{\partial \tilde{\theta}_j} = \frac{\partial |J|}{\partial \tilde{\theta}_j}\frac{1}{|J|}$$

And the gradient follows in the obvious manner.

We now combine this to have the log-posterior. Start with the posterior $\mathcal{P}$ and denote the prior by $P(\theta)$:

$$\mathcal{P}(\theta|Y) = L(\theta)P(\theta)
\log(\mathcal{P}(\theta|Y)) = \ell(\theta) + \log(P(\theta))$$

Notice that $P(\theta) = \prod_{i=1}^{p} P_i(\theta_i)$. Using the reparametrization, we have, for any given i:

$$P_i(\tilde{\theta}) = P_i(h_i^{-1}(\tilde{\theta}_i) \frac{dh^{-1}(\tilde{\theta}_i)}{d\tilde{\theta}_i}$$

So $\log(P(\theta)) = \sum_{i=1}^{p} \log(P_i(\theta_i))$ and for the reparametrization we will have:

$$\log(P(\tilde{\theta})) = \sum_{i=1}^{p} \log(P_i(\tilde{\theta}_i)) + \log\left(\frac{dh_i^{-1}(\tilde{\theta}_i)}{d\tilde{\theta}_i}\right)$$

So we can write the posterior as:

$$
\log(\mathcal{P}(\tilde{\theta}|Y)) = \ell(\tilde{\theta}) + \sum_{i=1}^{p} \log(P_i(\tilde{\theta}_i)) + \sum_{i=1}^{p} \log\left(\frac{dh_i^{-1}(\tilde{\theta}_i)}{d\tilde{\theta}_i}\right)
$$

Thats all done in the `renormalization_manual.jl` file.

The functions that I used for reparametrization are (those are $h^{-1}$, since they take the whole line and reduce it to the restricted area):

* Positive parameters: exp(x)
* Unit interval: $[0,1]$: 1/(1+exp(-x))
* Larger than 1: exp(x) + 1

The last transformation is applied to the inflation reaction parameter to guarantee that the region of non existence of equilibrium is never visited.

We can take the analytical derivates of the functions above. We implement it and the jacobian in Julia:

```{julia}

to_unit(x) = 1/(1+exp(-x))
to_positive(x) = exp(x)
to_one_inf(x) = exp(x) + 1

d_to_unit(x) = exp(x)/(exp(-x)+1)^2
d_to_positive(x) = exp(x)
d_to_one_inf(x) = exp(x)

function renormalization(par)

    J_1 = d_to_unit(par[2])
    J_2 = d_to_positive(par[3])
    J_3 = d_to_unit(par[4])
    J_4 = d_to_positive(par[5])
    J_5 = d_to_positive(par[6])
    J_6 = d_to_positive(par[7])
    J_7 = d_to_one_inf(par[8])
    J_8 = d_to_positive(par[9])
    J_9 = d_to_unit(par[10])

    return([J_1;J_2;J_3;J_4;J_5;J_6;J_7;J_8;J_9])
end
```
In which the parameter vector is $\left[\beta,\epsilon,\theta,\sigma,\sigma_u^2,\phi,\phi_{\pi},\phi_y,\rho_v\right]$

And now the function that takes the parameters defined in $\mathbb{R}^p$ and convert them to the loglikelihood and apply the correction of the Jacobian and the derivate of the jacobian to the loglikelihood and to the gradient, respectively:

```{julia}

function dens_and_grad(par,data)
    alfa = par[1]
    bet = to_unit(par[2])
    epsilon = to_positive(par[3])
    theta = to_unit(par[4])
    sig = to_positive(par[5])
    s2 = to_positive(par[6])
    phi = to_positive(par[7])
    phi_pi = to_one_inf(par[8])
    phi_y = to_positive(par[9])
    rho_v = to_unit(par[10])

    renorm = renormalization(par)
    jacob = abs(prod(renorm))

    renorm_diff = ForwardDiff.gradient(x->abs(prod(renormalization(x))),par) #derivate of abs det of Jacobian matrix

    diff_correction = 1/jacob*renorm_diff
    diff_correction = diff_correction[2:10]

    log_p_bet(bet) = logpdf(prior_bet,bet)
    log_p_epsilon(epsilon) = logpdf(prior_epsilon,epsilon)
    log_p_theta(theta) = logpdf(prior_theta,theta)
    log_p_sig(sig) = logpdf(prior_sig,sig)
    log_p_s2(s2) = logpdf(prior_s2,s2)
    log_p_phi(phi) = logpdf(prior_phi,phi)
    log_p_phi_pi(phi_pi) = logpdf(prior_phi_pi,phi_pi)
    log_p_phi_y(phi_y) = logpdf(prior_phi_y,phi_y)
    log_p_rho_v(rho_v) = logpdf(prior_rho_v,rho_v)

    llh,dll = log_like_dsge([alfa,bet,epsilon,theta,sig,s2,phi,phi_pi,phi_y,rho_v],data)
    llh = llh + size(data,1)*log(jacob) + log_p_bet(bet) + log_p_epsilon(epsilon) + log_p_theta(theta) + log_p_sig(sig) + log_p_s2(s2) + log_p_phi(phi) + log_p_phi_pi(phi_pi) + log_p_phi_y(phi_y) + log_p_rho_v(rho_v) + sum(log.(abs.(renorm)))
    dll = dll[2:10]
    dll = dll .*renorm + size(data,1)*diff_correction + [ForwardDiff.derivative(log_p_bet,bet); ForwardDiff.derivative(log_p_epsilon,epsilon); ForwardDiff.derivative(log_p_theta,theta); ForwardDiff.derivative(log_p_sig,sig); ForwardDiff.derivative(log_p_s2,s2); ForwardDiff.derivative(log_p_phi,phi); ForwardDiff.derivative(log_p_phi_pi,phi_pi); ForwardDiff.derivative(log_p_phi_y,phi_y); ForwardDiff.derivative(log_p_rho_v,rho_v);]

    return llh,dll
end
```
